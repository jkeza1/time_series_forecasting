{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkeza1/time_series_forecasting/blob/main/air_quality_forecasting_starter_code_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beijing Air Quality Forecasting Starter Notebook"
      ],
      "metadata": {
        "id": "iTsEYdtov6tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "nWkSHhqXrCqF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_submission(predictions, experiment_name, test_index):\n",
        "    \"\"\"Save submission with timestamp and experiment info\"\"\"\n",
        "    os.makedirs('submissions', exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # FIXED: Apply strftime to each datetime individually\n",
        "    row_ids = test_index.strftime('%Y-%m-%d %H:%M:%S')  # This now works correctly\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': row_ids,  # Use the properly formatted row IDs\n",
        "        'pm2.5': predictions.round().astype(int)\n",
        "    })\n",
        "\n",
        "    filename = f'submissions/{timestamp}_{experiment_name}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"âœ… Submission saved: {filename}\")\n",
        "    print(f\"ğŸ“Š Predictions - Min: {predictions.min():.1f}, Max: {predictions.max():.1f}\")\n",
        "    print(f\"ğŸ“‹ Number of row IDs: {len(row_ids)}\")  # Added for verification\n",
        "\n",
        "    return filename, submission"
      ],
      "metadata": {
        "id": "ckzBsJN_FRc1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_C4HV99rHd5",
        "outputId": "7cd7dcd4-b7da-4e51-a32b-48e6b8346b17"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "# Ensure train.csv and test.csv are saved in your Google Drive in the same folder.\n",
        "# Replace the file paths below with the actual paths to your dataset.\n",
        "train = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/test.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/sample_submission.csv')\n"
      ],
      "metadata": {
        "id": "gxW-6b_jrLAL"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ğŸ“Š Training data: {train.shape}\")\n",
        "print(f\"ğŸ“Š Test data: {test.shape}\")\n",
        "print(f\"\\nğŸ“‹ Columns: {list(train.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjYBx9PmFuUA",
        "outputId": "bebf99c6-9530-419b-bbd9-b1a89c377941"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Training data: (30676, 12)\n",
            "ğŸ“Š Test data: (13148, 11)\n",
            "\n",
            "ğŸ“‹ Columns: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "print(train.head())\n",
        "print(test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCaTq55zJp00",
        "outputId": "3fcf0b3c-4f8f-47b5-99eb-602889df9133"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0   1 -1.580878 -1.922250  0.443328 -0.441894 -0.069353 -0.137667   \n",
            "1   2 -1.580878 -2.004228  0.345943 -0.379306 -0.069353 -0.137667   \n",
            "2   3 -1.580878 -1.922250  0.248559 -0.343514 -0.069353 -0.137667   \n",
            "3   4 -1.580878 -2.168183  0.248559 -0.280926 -0.069353 -0.137667   \n",
            "4   5 -1.511594 -2.004228  0.151174 -0.218339 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  pm2.5  \n",
            "0  2010-01-01 00:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "1  2010-01-01 01:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "2  2010-01-01 02:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "3  2010-01-01 03:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "4  2010-01-01 04:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "      No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0  30677  1.190496  0.701029 -2.186052 -0.003982 -0.069353 -0.137667   \n",
            "1  30678  1.121211  0.619051 -2.186052  0.031811 -0.069353 -0.137667   \n",
            "2  30679  1.190496  0.783006 -2.186052  0.094398 -0.069353 -0.137667   \n",
            "3  30680  1.190496  0.946961 -2.088668  0.174782 -0.069353 -0.137667   \n",
            "4  30681  1.190496  1.192893 -1.991283  0.210575 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  \n",
            "0  2013-07-02 04:00:00  1.448138 -0.732019 -0.522096  \n",
            "1  2013-07-02 05:00:00  1.448138 -0.732019 -0.522096  \n",
            "2  2013-07-02 06:00:00  1.448138 -0.732019 -0.522096  \n",
            "3  2013-07-02 07:00:00  1.448138 -0.732019 -0.522096  \n",
            "4  2013-07-02 08:00:00  1.448138 -0.732019 -0.522096  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nâ“ Missing values:\")\n",
        "print(f\"Train: {train.isnull().sum().sum()}\")\n",
        "print(f\"Test: {test.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXnJf-BDFzJn",
        "outputId": "08f3878d-c278-48f7-90bd-769de2df5751"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â“ Missing values:\n",
            "Train: 1921\n",
            "Test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "train.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)"
      ],
      "metadata": {
        "id": "GDeAIfM8F9sl"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Time range - Train: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Time range - Test: {test.index.min()} to {test.index.max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIpUlu08GDKp",
        "outputId": "d3b54daf-c538-412e-8958-e80d810e85a4"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time range - Train: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Time range - Test: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Key insights\n",
        "if 'pm2.5' in train.columns:\n",
        "    print(f\"\\nPM2.5 statistics:\")\n",
        "    print(f\"Mean: {train['pm2.5'].mean():.1f}, Std: {train['pm2.5'].std():.1f}\")\n",
        "    print(f\"Min: {train['pm2.5'].min():.1f}, Max: {train['pm2.5'].max():.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDLXFukLGNcB",
        "outputId": "3bc7b15a-0388-458e-a1ba-a1bb65cb0251"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PM2.5 statistics:\n",
            "Mean: 100.8, Std: 93.1\n",
            "Min: 0.0, Max: 994.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the training data\n",
        "\n",
        "In this sections explore your dataset with appropiate statistics and visualisations to understand your better. Ensure that you explain output of every code cell and what it entails."
      ],
      "metadata": {
        "id": "cRse3uqRrft5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_features(df, target_col='pm2.5'):\n",
        "    \"\"\"Create lag features, rolling statistics, and temporal features\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    if target_col in df_enhanced.columns:\n",
        "        # Lag features for PM2.5\n",
        "        for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_lag_{lag}'] = df_enhanced[target_col].shift(lag)\n",
        "\n",
        "        # Rolling statistics for PM2.5\n",
        "        for window in [6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_roll_mean_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'pm2.5_roll_std_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).std()\n",
        "            df_enhanced[f'pm2.5_roll_min_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).min()\n",
        "            df_enhanced[f'pm2.5_roll_max_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).max()\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "\n",
        "    # Drop original temporal columns (keep encoded versions)\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features(df):\n",
        "    \"\"\"Create features for test data (without PM2.5 lag features)\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "\n",
        "    # Drop original temporal columns\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced"
      ],
      "metadata": {
        "id": "-M4xffncY8ZT"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = train.copy()\n",
        "# First fill with forward fill, then backward fill, then interpolate\n",
        "train_clean = train_clean.fillna(method='ffill', limit=12)  # Limit forward fill to 12 hours\n",
        "train_clean = train_clean.fillna(method='bfill', limit=12)  # Limit backward fill to 12 hours\n",
        "train_clean = train_clean.interpolate(method='time')"
      ],
      "metadata": {
        "id": "ke0d3XEZGeYh"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"âœ… After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"ğŸ“Š Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zplS87XlGhkM",
        "outputId": "13209dcb-ee08-4fd1-9adb-6d5f33999f26"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… After treatment - Train: 12, Test: 0\n",
            "ğŸ“Š Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = train.copy()\n",
        "train_clean = train_clean.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill').fillna(method='bfill').fillna(0)"
      ],
      "metadata": {
        "id": "bzY0pBkMiJE2"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\" Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "\n",
        "print(f\"Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R74CEBFrYok",
        "outputId": "af7f68da-8d0e-433c-e7af-6a3d9f4287f5"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Handling remaining NaN values...\n",
            "Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle missing values\n",
        "\n",
        "\n",
        "- Check the dataset for missing values and decide how to handle them.\n",
        "- In this example, missing values are filled with the mean. You can experiment with other strategies."
      ],
      "metadata": {
        "id": "ABAqt0Jztd5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "print(\"ğŸ”§ Handling missing values...\")\n",
        "\n",
        "# For training data\n",
        "train_clean = train.copy()\n",
        "train_clean = train_clean.fillna(method='ffill')  # Forward fill\n",
        "train_clean = train_clean.fillna(method='bfill')  # Backward fill\n",
        "train_clean = train_clean.interpolate(method='linear')  # Linear interpolation\n",
        "\n",
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"âœ… After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"ğŸ“Š Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "id": "u2n29Ge1tami",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc09eec8-9c4c-41c2-aa3c-b820c30f3106"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Handling missing values...\n",
            "âœ… After treatment - Train: 0, Test: 0\n",
            "ğŸ“Š Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\"ğŸ”§ Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)\n",
        "\n",
        "print(f\"âœ… Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXE3GFYn6PsP",
        "outputId": "e5bf4bf9-c543-4a7f-c065-8718e60bbf14"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Handling remaining NaN values...\n",
            "âœ… Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate features and target\n",
        "\n",
        "- Feel free to trop any non-essential columns like that you think might not contribute to modeling."
      ],
      "metadata": {
        "id": "YKndkdRuty1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time series sequence creation\n",
        "def create_sequences(data, target, sequence_length=24):\n",
        "    \"\"\"Create sequences for LSTM input\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "QETLRAo_tvQH"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# SEPARATE FEATURES & TARGET\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ¯ Separating features and target variables...\")\n",
        "\n",
        "# For training data - use the common features we already identified\n",
        "train_features = X_train_common  # This already has the common features only\n",
        "train_target = train_enhanced['pm2.5']\n",
        "\n",
        "# For test data - use the common features\n",
        "test_features = X_test_common    # This already has the common features only\n",
        "\n",
        "print(f\"ğŸ“Š Training features shape: {train_features.shape}\")\n",
        "print(f\"ğŸ“Š Training target shape: {train_target.shape}\")\n",
        "print(f\"ğŸ“Š Test features shape: {test_features.shape}\")\n",
        "\n",
        "# Display feature names\n",
        "print(f\"\\nğŸ” Feature columns: {list(train_features.columns)}\")\n",
        "print(f\"ğŸ¯ Target column: pm2.5\")"
      ],
      "metadata": {
        "id": "NyP2mDjruG9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa93787-bdc8-46c4-b726-2f61dead4e52"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Separating features and target variables...\n",
            "ğŸ“Š Training features shape: (30676, 45)\n",
            "ğŸ“Š Training target shape: (30676,)\n",
            "ğŸ“Š Test features shape: (13148, 45)\n",
            "\n",
            "ğŸ” Feature columns: ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ¯ Target column: pm2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ—ï¸ Creating time series sequences...\")\n",
        "X_seq, y_seq = create_sequences(X_train_scaled, train_target.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq))\n",
        "X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train_seq, y_val = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "print(f\"ğŸ“Š Sequences shape: {X_seq.shape}\")\n",
        "print(f\"ğŸ“Š Train: {X_train.shape}, Validation: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "TwaTWg66KY1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75619a94-82f2-49df-e146-b9e9f92a9bae"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—ï¸ Creating time series sequences...\n",
            "ğŸ“Š Sequences shape: (30640, 36, 45)\n",
            "ğŸ“Š Train: (26044, 36, 45), Validation: (4596, 36, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove PM2.5-dependent features from training\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_aligned)\n",
        "X_test_scaled = scaler.transform(X_test_aligned)\n",
        "\n",
        "print(\"âœ… Feature alignment completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU5uuOoNiTyb",
        "outputId": "d27e8466-af11-4066-8045-5cadc33ec6bb"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model\n",
        "\n",
        "Below is a simple LSTM model. Your task is to experiment with different parameters like, numbers of layers, units, activation functions, and optimizers, etc to get the best performing model. Experiment with other optimizers (e.g., SGD) or hyperparameters to improve performance."
      ],
      "metadata": {
        "id": "d488782wuR2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 36\n",
        "BATCH_SIZE = 128\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=15, restore_best_weights=True, min_delta=1e-4),\n",
        "    ReduceLROnPlateau(factor=0.7, patience=8, min_lr=1e-6, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "ewWI3IHKhZN5"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_submission(predictions, experiment_name, test_index):\n",
        "    \"\"\"Save submission with timestamp and experiment info\"\"\"\n",
        "    os.makedirs('submissions', exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': test_index.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'pm2.5': predictions.round().astype(int)\n",
        "    })\n",
        "\n",
        "    filename = f'submissions/{timestamp}_{experiment_name}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"âœ… Submission saved: {filename}\")\n",
        "    print(f\"ğŸ“Š Predictions - Min: {predictions.min():.1f}, Max: {predictions.max():.1f}\")\n",
        "\n",
        "    return filename, submission\n",
        "\n",
        "def download_submission(filename):\n",
        "    \"\"\"Download the submission file from Colab\"\"\"\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        files.download(filename)\n",
        "        print(f\"âœ… Download initiated: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Download failed: {e}\")\n",
        "        print(f\"ğŸ“ File is available at: {filename}\")\n",
        "        print(\"ğŸ’¡ You can manually download it from the file browser on the left\")\n",
        "\n",
        "def create_sequences(data, target, sequence_length=24):\n",
        "    \"\"\"Create sequences for LSTM input\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def create_enhanced_model(input_shape):\n",
        "    \"\"\"Create enhanced LSTM model\"\"\"\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(64, activation='tanh', return_sequences=True), input_shape=input_shape),\n",
        "        Dropout(0.4),\n",
        "        LSTM(32, activation='tanh', return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        LSTM(16, activation='tanh'),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_advanced_features(df, target_col='pm2.5'):\n",
        "    \"\"\"Create lag features, rolling statistics, and temporal features\"\"\"\n",
        "    # ... [your existing feature engineering code] ...\n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features(df):\n",
        "    \"\"\"Create features for test data (without PM2.5 lag features)\"\"\"\n",
        "    # ... [your existing test feature engineering code] ...\n",
        "    return df_enhanced\n",
        "\n",
        "print(\"All functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY0EY5gIhrbn",
        "outputId": "ab7dc3aa-1335-4255-8ef9-ea0ce2c08307"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# RE-TRAIN WITH ALIGNED FEATURES (QUICK VERSION)\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ”„ Training model with aligned features...\")\n",
        "\n",
        "# Create sequences with aligned features\n",
        "X_seq_aligned, y_seq_aligned = create_sequences(X_train_scaled_aligned, y_train.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq_aligned))\n",
        "X_train_aligned, X_val_aligned = X_seq_aligned[:split_idx], X_seq_aligned[split_idx:]\n",
        "y_train_aligned, y_val_aligned = y_seq_aligned[:split_idx], y_seq_aligned[split_idx:]\n",
        "\n",
        "print(f\"ğŸ“Š Aligned sequences shape: {X_seq_aligned.shape}\")\n",
        "\n",
        "# Build model with correct input shape\n",
        "model_aligned = create_enhanced_model(X_train_aligned.shape[1:])\n",
        "model_aligned.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train quickly (fewer epochs since we already know good parameters)\n",
        "history_aligned = model_aligned.fit(\n",
        "    X_train_aligned, y_train_aligned,\n",
        "    validation_data=(X_val_aligned, y_val_aligned),\n",
        "    epochs=30,  # Fewer epochs for quick training\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_pred_aligned = model_aligned.predict(X_val_aligned, verbose=0)\n",
        "val_rmse_aligned = np.sqrt(mean_squared_error(y_val_aligned, val_pred_aligned))\n",
        "print(f\" Aligned Model RMSE: {val_rmse_aligned:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgGEfdIwXwL7",
        "outputId": "fa1071b2-8485-454b-b586-6a2bb629d932"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Training model with aligned features...\n",
            "ğŸ“Š Aligned sequences shape: (30640, 36, 45)\n",
            "Epoch 1/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 181ms/step - loss: 83.3467 - mae: 83.8450 - val_loss: 76.8963 - val_mae: 77.3946 - learning_rate: 0.0020\n",
            "Epoch 2/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 197ms/step - loss: 51.1861 - mae: 51.6820 - val_loss: 48.7370 - val_mae: 49.2326 - learning_rate: 0.0020\n",
            "Epoch 3/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 185ms/step - loss: 35.2505 - mae: 35.7447 - val_loss: 49.4234 - val_mae: 49.9193 - learning_rate: 0.0020\n",
            "Epoch 4/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 191ms/step - loss: 31.2208 - mae: 31.7148 - val_loss: 45.0255 - val_mae: 45.5215 - learning_rate: 0.0020\n",
            "Epoch 5/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 179ms/step - loss: 28.2710 - mae: 28.7646 - val_loss: 47.4143 - val_mae: 47.9100 - learning_rate: 0.0020\n",
            "Epoch 6/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 183ms/step - loss: 26.2409 - mae: 26.7342 - val_loss: 45.3620 - val_mae: 45.8578 - learning_rate: 0.0020\n",
            "Epoch 7/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 188ms/step - loss: 24.0627 - mae: 24.5545 - val_loss: 46.7598 - val_mae: 47.2553 - learning_rate: 0.0020\n",
            "Epoch 8/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 191ms/step - loss: 22.7599 - mae: 23.2522 - val_loss: 46.0503 - val_mae: 46.5464 - learning_rate: 0.0020\n",
            "Epoch 9/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 205ms/step - loss: 21.8709 - mae: 22.3623 - val_loss: 48.3059 - val_mae: 48.8008 - learning_rate: 0.0020\n",
            "Epoch 10/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 183ms/step - loss: 20.8391 - mae: 21.3308 - val_loss: 48.6482 - val_mae: 49.1437 - learning_rate: 0.0020\n",
            "Epoch 11/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 179ms/step - loss: 19.9731 - mae: 20.4648 - val_loss: 48.1549 - val_mae: 48.6504 - learning_rate: 0.0020\n",
            "Epoch 12/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 19.2985 - mae: 19.7896\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - loss: 19.2977 - mae: 19.7887 - val_loss: 47.2275 - val_mae: 47.7218 - learning_rate: 0.0020\n",
            "Epoch 13/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 179ms/step - loss: 18.2896 - mae: 18.7789 - val_loss: 47.7764 - val_mae: 48.2717 - learning_rate: 0.0014\n",
            "Epoch 14/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 205ms/step - loss: 17.4492 - mae: 17.9396 - val_loss: 48.5195 - val_mae: 49.0148 - learning_rate: 0.0014\n",
            "Epoch 15/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 188ms/step - loss: 17.0729 - mae: 17.5628 - val_loss: 47.5775 - val_mae: 48.0725 - learning_rate: 0.0014\n",
            "Epoch 16/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 199ms/step - loss: 16.6922 - mae: 17.1821 - val_loss: 47.5271 - val_mae: 48.0224 - learning_rate: 0.0014\n",
            "Epoch 17/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 217ms/step - loss: 16.4676 - mae: 16.9568 - val_loss: 48.4707 - val_mae: 48.9653 - learning_rate: 0.0014\n",
            "Epoch 18/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 204ms/step - loss: 16.2338 - mae: 16.7231 - val_loss: 48.0253 - val_mae: 48.5196 - learning_rate: 0.0014\n",
            "Epoch 19/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 181ms/step - loss: 16.0610 - mae: 16.5505 - val_loss: 48.6598 - val_mae: 49.1553 - learning_rate: 0.0014\n",
            " Aligned Model RMSE: 74.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# GENERATE PREDICTIONS (WILL WORK NOW)\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ”® Generating test predictions with aligned features...\")\n",
        "\n",
        "test_sequences = []\n",
        "for i in range(len(X_test_scaled_aligned)):\n",
        "    if i < SEQUENCE_LENGTH:\n",
        "        needed_from_train = SEQUENCE_LENGTH - (i + 1)\n",
        "        if needed_from_train > 0:\n",
        "            sequence = np.vstack([X_train_scaled_aligned[-needed_from_train:], X_test_scaled_aligned[:i+1]])\n",
        "        else:\n",
        "            sequence = X_test_scaled_aligned[:SEQUENCE_LENGTH]\n",
        "    else:\n",
        "        sequence = X_test_scaled_aligned[i-SEQUENCE_LENGTH+1:i+1]\n",
        "\n",
        "    if sequence.shape[0] != SEQUENCE_LENGTH:\n",
        "        if sequence.shape[0] < SEQUENCE_LENGTH:\n",
        "            padding_needed = SEQUENCE_LENGTH - sequence.shape[0]\n",
        "            padding = np.repeat(sequence[0:1], padding_needed, axis=0)\n",
        "            sequence = np.vstack([padding, sequence])\n",
        "        else:\n",
        "            sequence = sequence[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    test_sequences.append(sequence)\n",
        "\n",
        "X_test_seq = np.array(test_sequences)\n",
        "print(f\"ğŸ“Š Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "# This will work now - same feature dimensions!\n",
        "test_predictions = model_aligned.predict(X_test_seq, verbose=0)\n",
        "test_predictions = np.maximum(test_predictions.flatten(), 0)\n",
        "\n",
        "print(f\"ğŸ“Š Predictions - Min: {test_predictions.min():.1f}, Max: {test_predictions.max():.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewGYEfVeXyWa",
        "outputId": "04410c2d-d352-4a55-d35a-beee6cd23024"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”® Generating test predictions with aligned features...\n",
            "ğŸ“Š Test sequences shape: (13148, 36, 45)\n",
            "ğŸ“Š Predictions - Min: 6.0, Max: 355.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸš€ Building and training LSTM model...\")\n",
        "\n",
        "# Create and train model\n",
        "model = create_enhanced_model(X_train.shape[1:])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_seq,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"âœ… Model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1_J-DptOGx2",
        "outputId": "f8b62bf8-8fd1-43d7-87c2-49c690583639"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Building and training LSTM model...\n",
            "Epoch 1/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 195ms/step - loss: 82.9128 - mae: 83.4111 - val_loss: 58.5081 - val_mae: 59.0051 - learning_rate: 0.0020\n",
            "Epoch 2/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 206ms/step - loss: 44.7753 - mae: 45.2711 - val_loss: 51.8344 - val_mae: 52.3311 - learning_rate: 0.0020\n",
            "Epoch 3/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 188ms/step - loss: 37.1888 - mae: 37.6836 - val_loss: 47.8030 - val_mae: 48.2966 - learning_rate: 0.0020\n",
            "Epoch 4/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - loss: 33.3223 - mae: 33.8163 - val_loss: 45.6550 - val_mae: 46.1499 - learning_rate: 0.0020\n",
            "Epoch 5/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 206ms/step - loss: 30.5232 - mae: 31.0170 - val_loss: 47.7789 - val_mae: 48.2737 - learning_rate: 0.0020\n",
            "Epoch 6/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 184ms/step - loss: 28.7558 - mae: 29.2500 - val_loss: 46.0650 - val_mae: 46.5598 - learning_rate: 0.0020\n",
            "Epoch 7/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 202ms/step - loss: 26.5616 - mae: 27.0549 - val_loss: 45.6625 - val_mae: 46.1588 - learning_rate: 0.0020\n",
            "Epoch 8/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 180ms/step - loss: 24.7060 - mae: 25.1987 - val_loss: 46.5441 - val_mae: 47.0399 - learning_rate: 0.0020\n",
            "Epoch 9/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 200ms/step - loss: 23.3387 - mae: 23.8305 - val_loss: 46.6396 - val_mae: 47.1352 - learning_rate: 0.0020\n",
            "Epoch 10/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 178ms/step - loss: 22.0664 - mae: 22.5585 - val_loss: 47.0664 - val_mae: 47.5612 - learning_rate: 0.0020\n",
            "Epoch 11/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 173ms/step - loss: 21.2582 - mae: 21.7499 - val_loss: 48.7263 - val_mae: 49.2221 - learning_rate: 0.0020\n",
            "Epoch 12/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 20.3921 - mae: 20.8840\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 194ms/step - loss: 20.3901 - mae: 20.8821 - val_loss: 48.3256 - val_mae: 48.8196 - learning_rate: 0.0020\n",
            "Epoch 13/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 189ms/step - loss: 19.1157 - mae: 19.6068 - val_loss: 48.6218 - val_mae: 49.1159 - learning_rate: 0.0014\n",
            "Epoch 14/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 176ms/step - loss: 18.3148 - mae: 18.8059 - val_loss: 48.9101 - val_mae: 49.4039 - learning_rate: 0.0014\n",
            "Epoch 15/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 194ms/step - loss: 17.8806 - mae: 18.3714 - val_loss: 49.2586 - val_mae: 49.7541 - learning_rate: 0.0014\n",
            "Epoch 16/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 190ms/step - loss: 17.4520 - mae: 17.9427 - val_loss: 46.5412 - val_mae: 47.0363 - learning_rate: 0.0014\n",
            "Epoch 17/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 193ms/step - loss: 16.7793 - mae: 17.2702 - val_loss: 49.6415 - val_mae: 50.1362 - learning_rate: 0.0014\n",
            "Epoch 18/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 208ms/step - loss: 16.7048 - mae: 17.1948 - val_loss: 49.1552 - val_mae: 49.6501 - learning_rate: 0.0014\n",
            "Epoch 19/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 177ms/step - loss: 16.7590 - mae: 17.2493 - val_loss: 49.3169 - val_mae: 49.8115 - learning_rate: 0.0014\n",
            "âœ… Model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“Š Evaluating model performance...\")\n",
        "\n",
        "# Evaluate model\n",
        "val_pred = model.predict(X_val, verbose=0)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "print(f\"\\nğŸ¯ Model Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Target: < 3000 RMSE\")\n",
        "\n",
        "if val_rmse < 3000:\n",
        "    print(\"ğŸ‰ TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"ğŸ“ˆ Need {val_rmse - 3000:.1f} points improvement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr6P3vWfHlCH",
        "outputId": "d69ed0e9-ab7f-45f8-f367-dceba3b03d89"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Evaluating model performance...\n",
            "\n",
            "ğŸ¯ Model Performance:\n",
            "Validation RMSE: 75.05\n",
            "Target: < 3000 RMSE\n",
            "ğŸ‰ TARGET ACHIEVED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CORRECT FEATURE ALIGNMENT\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ”§ Correcting feature alignment...\")\n",
        "\n",
        "# First, let's see what features we actually have\n",
        "print(\"ğŸ“Š Training features:\", train_enhanced.columns.tolist())\n",
        "print(\"ğŸ“Š Test features:\", test_enhanced.columns.tolist())\n",
        "\n",
        "# The problem: Training has PM2.5 lag features that test can't have\n",
        "# Solution: Remove PM2.5-dependent features from training\n",
        "pm25_dependent_features = [col for col in train_enhanced.columns if 'pm2.5_' in col]\n",
        "print(f\"ğŸš« Removing PM2.5-dependent features: {pm25_dependent_features}\")\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "print(f\"âœ… Using {len(common_features)} common features:\")\n",
        "print(common_features)\n",
        "\n",
        "# Create aligned datasets\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "print(f\"ğŸ“Š Aligned training features: {X_train_aligned.shape}\")\n",
        "print(f\"ğŸ“Š Aligned test features: {X_test_aligned.shape}\")\n",
        "\n",
        "# Scale the aligned features\n",
        "scaler_aligned = StandardScaler()\n",
        "X_train_scaled_aligned = scaler_aligned.fit_transform(X_train_aligned)\n",
        "X_test_scaled_aligned = scaler_aligned.transform(X_test_aligned)\n",
        "\n",
        "print(\"âœ… Feature alignment completed!\")"
      ],
      "metadata": {
        "id": "0SDDX2L1VF_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447749f2-d1aa-4172-e7a4-51c37dd392a7"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Correcting feature alignment...\n",
            "ğŸ“Š Training features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ“Š Test features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸš« Removing PM2.5-dependent features: ['pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48']\n",
            "âœ… Using 45 common features:\n",
            "['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ“Š Aligned training features: (30676, 45)\n",
            "ğŸ“Š Aligned test features: (13148, 45)\n",
            "âœ… Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CREATE SUBMISSION\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ’¾ Creating submission file...\")\n",
        "\n",
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ¯ EXPERIMENT COMPLETE!\")\n",
        "print(f\"ğŸ“ˆ Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"ğŸ’¾ Submission saved: {filename}\")\n",
        "print(f\"\\nğŸ“‹ Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"âœ… Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"ğŸ“ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"âŒ Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-U4WGAH1sE",
        "outputId": "0773edc4-1139-45c3-a7f8-924355bb9367"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Creating submission file...\n",
            "âœ… Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "ğŸ“Š Predictions - Min: 6.0, Max: 355.0\n",
            "\n",
            "ğŸ¯ EXPERIMENT COMPLETE!\n",
            "ğŸ“ˆ Validation RMSE: 75.05\n",
            "ğŸ’¾ Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "\n",
            "ğŸ“‹ Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0  2013-07-02 04:00:00     17\n",
            "1  2013-07-02 05:00:00     16\n",
            "2  2013-07-02 06:00:00     16\n",
            "3  2013-07-02 07:00:00     16\n",
            "4  2013-07-02 08:00:00     16\n",
            "5  2013-07-02 09:00:00     16\n",
            "6  2013-07-02 10:00:00     15\n",
            "7  2013-07-02 11:00:00     15\n",
            "8  2013-07-02 12:00:00     15\n",
            "9  2013-07-02 13:00:00     16\n",
            "âœ… Confirmed: submissions/20250920_104754_enhanced_lstm_rmse_75.csv exists in submissions folder!\n",
            "ğŸ“ File size: 305933 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ¯ EXPERIMENT COMPLETE!\")\n",
        "print(f\"ğŸ“ˆ Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"ğŸ’¾ Submission saved: {filename}\")\n",
        "print(f\"\\nğŸ“‹ Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"âœ… Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"ğŸ“ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"âŒ Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_YV-N-gJBAv",
        "outputId": "75537d80-701c-4788-d099-f034606aebd1"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "ğŸ“Š Predictions - Min: 6.0, Max: 355.0\n",
            "\n",
            "ğŸ¯ EXPERIMENT COMPLETE!\n",
            "ğŸ“ˆ Validation RMSE: 75.05\n",
            "ğŸ’¾ Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "\n",
            "ğŸ“‹ Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0  2013-07-02 04:00:00     17\n",
            "1  2013-07-02 05:00:00     16\n",
            "2  2013-07-02 06:00:00     16\n",
            "3  2013-07-02 07:00:00     16\n",
            "4  2013-07-02 08:00:00     16\n",
            "5  2013-07-02 09:00:00     16\n",
            "6  2013-07-02 10:00:00     15\n",
            "7  2013-07-02 11:00:00     15\n",
            "8  2013-07-02 12:00:00     15\n",
            "9  2013-07-02 13:00:00     16\n",
            "âœ… Confirmed: submissions/20250920_104754_enhanced_lstm_rmse_75.csv exists in submissions folder!\n",
            "ğŸ“ File size: 305933 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(test_predictions, experiment_name, test.index)\n",
        "\n",
        "# DOWNLOAD THE SUBMISSION FILE\n",
        "download_submission(filename)\n",
        "\n",
        "print(f\"\\nğŸ¯ EXPERIMENT COMPLETE!\")\n",
        "print(f\"ğŸ“ˆ Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"ğŸ’¾ Submission ready for Kaggle: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "G0iTXqeWjCyk",
        "outputId": "bcdd98ef-031c-4422-ebf9-ebe8684d035b"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "ğŸ“Š Predictions - Min: 6.0, Max: 355.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_404d6766-8db3-42b1-ab62-cfc30ab52e59\", \"20250920_104754_enhanced_lstm_rmse_75.csv\", 305933)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Download initiated: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "\n",
            "ğŸ¯ EXPERIMENT COMPLETE!\n",
            "ğŸ“ˆ Validation RMSE: 75.05\n",
            "ğŸ’¾ Submission ready for Kaggle: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data - Use the aligned features that match your training\n",
        "X_test = test_enhanced[common_features]  # Use the common features from alignment\n",
        "\n",
        "# Scale the test data using the same scaler you used for training\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create sequences for test data (same method as used in training)\n",
        "test_sequences = []\n",
        "for i in range(len(X_test_scaled)):\n",
        "    if i < SEQUENCE_LENGTH:\n",
        "        needed_from_train = SEQUENCE_LENGTH - (i + 1)\n",
        "        if needed_from_train > 0:\n",
        "            sequence = np.vstack([X_train_scaled[-needed_from_train:], X_test_scaled[:i+1]])\n",
        "        else:\n",
        "            sequence = X_test_scaled[:SEQUENCE_LENGTH]\n",
        "    else:\n",
        "        sequence = X_test_scaled[i-SEQUENCE_LENGTH+1:i+1]\n",
        "\n",
        "    if sequence.shape[0] != SEQUENCE_LENGTH:\n",
        "        if sequence.shape[0] < SEQUENCE_LENGTH:\n",
        "            padding_needed = SEQUENCE_LENGTH - sequence.shape[0]\n",
        "            padding = np.repeat(sequence[0:1], padding_needed, axis=0)\n",
        "            sequence = np.vstack([padding, sequence])\n",
        "        else:\n",
        "            sequence = sequence[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    test_sequences.append(sequence)\n",
        "\n",
        "X_test_seq = np.array(test_sequences)\n",
        "\n",
        "# Make predictions on the test set using the aligned model\n",
        "predictions = model_aligned.predict(X_test_seq, verbose=0)\n",
        "\n",
        "# Ensure predictions do not contain NaN values and are non-negative\n",
        "predictions = np.nan_to_num(predictions)\n",
        "predictions = np.maximum(predictions.flatten(), 0)\n",
        "\n",
        "# Convert predictions to integers\n",
        "predictions = np.round(predictions).astype(int)\n",
        "\n",
        "# Prepare the submission file - Use the exact same format as your working submission\n",
        "# Get the row IDs from your test index (already properly formatted in your working code)\n",
        "row_ids = test.index.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'row ID': row_ids,  # Use the same formatting that worked before\n",
        "    'pm2.5': predictions\n",
        "})\n",
        "\n",
        "# Save the file in CSV format for submission on Kaggle\n",
        "submission.to_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/subm_fixed.csv', index=False)\n",
        "\n",
        "print(f\"âœ… Submission saved with {len(predictions)} predictions\")\n",
        "print(f\"ğŸ“Š Predictions range: {predictions.min()} to {predictions.max()}\")\n",
        "print(f\"ğŸ“‹ Sample row IDs: {row_ids[:3].tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xj1KLZauUK8",
        "outputId": "ab3e7a9c-0d28-40c5-e378-268303b771a1"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Submission saved with 13148 predictions\n",
            "ğŸ“Š Predictions range: 6 to 355\n",
            "ğŸ“‹ Sample row IDs: ['2013-07-02 04:00:00', '2013-07-02 05:00:00', '2013-07-02 06:00:00']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HW5Oi1kiel57"
      }
    }
  ]
}