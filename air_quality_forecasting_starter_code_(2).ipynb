{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkeza1/time_series_forecasting/blob/main/air_quality_forecasting_starter_code_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beijing Air Quality Forecasting Starter Notebook"
      ],
      "metadata": {
        "id": "iTsEYdtov6tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "nWkSHhqXrCqF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_submission(predictions, experiment_name, test_index):\n",
        "    \"\"\"Save submission with timestamp and experiment info\"\"\"\n",
        "    os.makedirs('submissions', exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # FIXED: Apply strftime to each datetime individually\n",
        "    row_ids = test_index.strftime('%Y-%m-%d %H:%M:%S')  # This now works correctly\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': row_ids,  # Use the properly formatted row IDs\n",
        "        'pm2.5': predictions.round().astype(int)\n",
        "    })\n",
        "\n",
        "    filename = f'submissions/{timestamp}_{experiment_name}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Submission saved: {filename}\")\n",
        "    print(f\"üìä Predictions - Min: {predictions.min():.1f}, Max: {predictions.max():.1f}\")\n",
        "    print(f\"üìã Number of row IDs: {len(row_ids)}\")  # Added for verification\n",
        "\n",
        "    return filename, submission"
      ],
      "metadata": {
        "id": "ckzBsJN_FRc1"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_C4HV99rHd5",
        "outputId": "7cd7dcd4-b7da-4e51-a32b-48e6b8346b17"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "# Ensure train.csv and test.csv are saved in your Google Drive in the same folder.\n",
        "# Replace the file paths below with the actual paths to your dataset.\n",
        "train = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/test.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/sample_submission.csv')\n"
      ],
      "metadata": {
        "id": "gxW-6b_jrLAL"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"üìä Training data: {train.shape}\")\n",
        "print(f\"üìä Test data: {test.shape}\")\n",
        "print(f\"\\nüìã Columns: {list(train.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjYBx9PmFuUA",
        "outputId": "bebf99c6-9530-419b-bbd9-b1a89c377941"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Training data: (30676, 12)\n",
            "üìä Test data: (13148, 11)\n",
            "\n",
            "üìã Columns: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "print(train.head())\n",
        "print(test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCaTq55zJp00",
        "outputId": "3fcf0b3c-4f8f-47b5-99eb-602889df9133"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0   1 -1.580878 -1.922250  0.443328 -0.441894 -0.069353 -0.137667   \n",
            "1   2 -1.580878 -2.004228  0.345943 -0.379306 -0.069353 -0.137667   \n",
            "2   3 -1.580878 -1.922250  0.248559 -0.343514 -0.069353 -0.137667   \n",
            "3   4 -1.580878 -2.168183  0.248559 -0.280926 -0.069353 -0.137667   \n",
            "4   5 -1.511594 -2.004228  0.151174 -0.218339 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  pm2.5  \n",
            "0  2010-01-01 00:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "1  2010-01-01 01:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "2  2010-01-01 02:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "3  2010-01-01 03:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "4  2010-01-01 04:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "      No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0  30677  1.190496  0.701029 -2.186052 -0.003982 -0.069353 -0.137667   \n",
            "1  30678  1.121211  0.619051 -2.186052  0.031811 -0.069353 -0.137667   \n",
            "2  30679  1.190496  0.783006 -2.186052  0.094398 -0.069353 -0.137667   \n",
            "3  30680  1.190496  0.946961 -2.088668  0.174782 -0.069353 -0.137667   \n",
            "4  30681  1.190496  1.192893 -1.991283  0.210575 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  \n",
            "0  2013-07-02 04:00:00  1.448138 -0.732019 -0.522096  \n",
            "1  2013-07-02 05:00:00  1.448138 -0.732019 -0.522096  \n",
            "2  2013-07-02 06:00:00  1.448138 -0.732019 -0.522096  \n",
            "3  2013-07-02 07:00:00  1.448138 -0.732019 -0.522096  \n",
            "4  2013-07-02 08:00:00  1.448138 -0.732019 -0.522096  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n‚ùì Missing values:\")\n",
        "print(f\"Train: {train.isnull().sum().sum()}\")\n",
        "print(f\"Test: {test.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXnJf-BDFzJn",
        "outputId": "08f3878d-c278-48f7-90bd-769de2df5751"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùì Missing values:\n",
            "Train: 1921\n",
            "Test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "train.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)"
      ],
      "metadata": {
        "id": "GDeAIfM8F9sl"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Time range - Train: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Time range - Test: {test.index.min()} to {test.index.max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIpUlu08GDKp",
        "outputId": "d3b54daf-c538-412e-8958-e80d810e85a4"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time range - Train: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Time range - Test: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Key insights\n",
        "if 'pm2.5' in train.columns:\n",
        "    print(f\"\\nPM2.5 statistics:\")\n",
        "    print(f\"Mean: {train['pm2.5'].mean():.1f}, Std: {train['pm2.5'].std():.1f}\")\n",
        "    print(f\"Min: {train['pm2.5'].min():.1f}, Max: {train['pm2.5'].max():.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDLXFukLGNcB",
        "outputId": "3bc7b15a-0388-458e-a1ba-a1bb65cb0251"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PM2.5 statistics:\n",
            "Mean: 100.8, Std: 93.1\n",
            "Min: 0.0, Max: 994.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the training data\n",
        "\n",
        "In this sections explore your dataset with appropiate statistics and visualisations to understand your better. Ensure that you explain output of every code cell and what it entails."
      ],
      "metadata": {
        "id": "cRse3uqRrft5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_features(df, target_col='pm2.5'):\n",
        "    \"\"\"Create lag features, rolling statistics, and temporal features\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    if target_col in df_enhanced.columns:\n",
        "        # Lag features for PM2.5\n",
        "        for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_lag_{lag}'] = df_enhanced[target_col].shift(lag)\n",
        "\n",
        "        # Rolling statistics for PM2.5\n",
        "        for window in [6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_roll_mean_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'pm2.5_roll_std_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).std()\n",
        "            df_enhanced[f'pm2.5_roll_min_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).min()\n",
        "            df_enhanced[f'pm2.5_roll_max_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).max()\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "\n",
        "    # Drop original temporal columns (keep encoded versions)\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features(df):\n",
        "    \"\"\"Create features for test data (without PM2.5 lag features)\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "\n",
        "    # Drop original temporal columns\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced"
      ],
      "metadata": {
        "id": "-M4xffncY8ZT"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = train.copy()\n",
        "# First fill with forward fill, then backward fill, then interpolate\n",
        "train_clean = train_clean.fillna(method='ffill', limit=12)  # Limit forward fill to 12 hours\n",
        "train_clean = train_clean.fillna(method='bfill', limit=12)  # Limit backward fill to 12 hours\n",
        "train_clean = train_clean.interpolate(method='time')"
      ],
      "metadata": {
        "id": "ke0d3XEZGeYh"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"‚úÖ After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"üìä Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zplS87XlGhkM",
        "outputId": "13209dcb-ee08-4fd1-9adb-6d5f33999f26"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ After treatment - Train: 12, Test: 0\n",
            "üìä Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = train.copy()\n",
        "train_clean = train_clean.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill').fillna(method='bfill').fillna(0)"
      ],
      "metadata": {
        "id": "bzY0pBkMiJE2"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\" Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "\n",
        "print(f\"Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R74CEBFrYok",
        "outputId": "af7f68da-8d0e-433c-e7af-6a3d9f4287f5"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Handling remaining NaN values...\n",
            "Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle missing values\n",
        "\n",
        "\n",
        "- Check the dataset for missing values and decide how to handle them.\n",
        "- In this example, missing values are filled with the mean. You can experiment with other strategies."
      ],
      "metadata": {
        "id": "ABAqt0Jztd5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "print(\"üîß Handling missing values...\")\n",
        "\n",
        "# For training data\n",
        "train_clean = train.copy()\n",
        "train_clean = train_clean.fillna(method='ffill')  # Forward fill\n",
        "train_clean = train_clean.fillna(method='bfill')  # Backward fill\n",
        "train_clean = train_clean.interpolate(method='linear')  # Linear interpolation\n",
        "\n",
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"‚úÖ After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"üìä Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "id": "u2n29Ge1tami",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc09eec8-9c4c-41c2-aa3c-b820c30f3106"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Handling missing values...\n",
            "‚úÖ After treatment - Train: 0, Test: 0\n",
            "üìä Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\"üîß Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)\n",
        "\n",
        "print(f\"‚úÖ Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXE3GFYn6PsP",
        "outputId": "e5bf4bf9-c543-4a7f-c065-8718e60bbf14"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Handling remaining NaN values...\n",
            "‚úÖ Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate features and target\n",
        "\n",
        "- Feel free to trop any non-essential columns like that you think might not contribute to modeling."
      ],
      "metadata": {
        "id": "YKndkdRuty1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time series sequence creation\n",
        "def create_sequences(data, target, sequence_length=24):\n",
        "    \"\"\"Create sequences for LSTM input\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "QETLRAo_tvQH"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# SEPARATE FEATURES & TARGET\n",
        "# ======================\n",
        "\n",
        "print(\"üéØ Separating features and target variables...\")\n",
        "\n",
        "# For training data - use the common features we already identified\n",
        "train_features = X_train_common  # This already has the common features only\n",
        "train_target = train_enhanced['pm2.5']\n",
        "\n",
        "# For test data - use the common features\n",
        "test_features = X_test_common    # This already has the common features only\n",
        "\n",
        "print(f\"üìä Training features shape: {train_features.shape}\")\n",
        "print(f\"üìä Training target shape: {train_target.shape}\")\n",
        "print(f\"üìä Test features shape: {test_features.shape}\")\n",
        "\n",
        "# Display feature names\n",
        "print(f\"\\nüîç Feature columns: {list(train_features.columns)}\")\n",
        "print(f\"üéØ Target column: pm2.5\")"
      ],
      "metadata": {
        "id": "NyP2mDjruG9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa93787-bdc8-46c4-b726-2f61dead4e52"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Separating features and target variables...\n",
            "üìä Training features shape: (30676, 45)\n",
            "üìä Training target shape: (30676,)\n",
            "üìä Test features shape: (13148, 45)\n",
            "\n",
            "üîç Feature columns: ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üéØ Target column: pm2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üèóÔ∏è Creating time series sequences...\")\n",
        "X_seq, y_seq = create_sequences(X_train_scaled, train_target.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq))\n",
        "X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train_seq, y_val = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "print(f\"üìä Sequences shape: {X_seq.shape}\")\n",
        "print(f\"üìä Train: {X_train.shape}, Validation: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "TwaTWg66KY1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75619a94-82f2-49df-e146-b9e9f92a9bae"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèóÔ∏è Creating time series sequences...\n",
            "üìä Sequences shape: (30640, 36, 45)\n",
            "üìä Train: (26044, 36, 45), Validation: (4596, 36, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove PM2.5-dependent features from training\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_aligned)\n",
        "X_test_scaled = scaler.transform(X_test_aligned)\n",
        "\n",
        "print(\"‚úÖ Feature alignment completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU5uuOoNiTyb",
        "outputId": "d27e8466-af11-4066-8045-5cadc33ec6bb"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model\n",
        "\n",
        "Below is a simple LSTM model. Your task is to experiment with different parameters like, numbers of layers, units, activation functions, and optimizers, etc to get the best performing model. Experiment with other optimizers (e.g., SGD) or hyperparameters to improve performance."
      ],
      "metadata": {
        "id": "d488782wuR2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 36\n",
        "BATCH_SIZE = 128\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=15, restore_best_weights=True, min_delta=1e-4),\n",
        "    ReduceLROnPlateau(factor=0.7, patience=8, min_lr=1e-6, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "ewWI3IHKhZN5"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_submission(predictions, experiment_name, test_index):\n",
        "    \"\"\"Save submission with timestamp and experiment info\"\"\"\n",
        "    os.makedirs('submissions', exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': test_index.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'pm2.5': predictions.round().astype(int)\n",
        "    })\n",
        "\n",
        "    filename = f'submissions/{timestamp}_{experiment_name}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Submission saved: {filename}\")\n",
        "    print(f\"üìä Predictions - Min: {predictions.min():.1f}, Max: {predictions.max():.1f}\")\n",
        "\n",
        "    return filename, submission\n",
        "\n",
        "def download_submission(filename):\n",
        "    \"\"\"Download the submission file from Colab\"\"\"\n",
        "    from google.colab import files\n",
        "    try:\n",
        "        files.download(filename)\n",
        "        print(f\"‚úÖ Download initiated: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download failed: {e}\")\n",
        "        print(f\"üìÅ File is available at: {filename}\")\n",
        "        print(\"üí° You can manually download it from the file browser on the left\")\n",
        "\n",
        "def create_sequences(data, target, sequence_length=24):\n",
        "    \"\"\"Create sequences for LSTM input\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def create_enhanced_model(input_shape):\n",
        "    \"\"\"Create enhanced LSTM model\"\"\"\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(64, activation='tanh', return_sequences=True), input_shape=input_shape),\n",
        "        Dropout(0.4),\n",
        "        LSTM(32, activation='tanh', return_sequences=True),\n",
        "        Dropout(0.3),\n",
        "        LSTM(16, activation='tanh'),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_advanced_features(df, target_col='pm2.5'):\n",
        "    \"\"\"Create lag features, rolling statistics, and temporal features\"\"\"\n",
        "    # ... [your existing feature engineering code] ...\n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features(df):\n",
        "    \"\"\"Create features for test data (without PM2.5 lag features)\"\"\"\n",
        "    # ... [your existing test feature engineering code] ...\n",
        "    return df_enhanced\n",
        "\n",
        "print(\"All functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY0EY5gIhrbn",
        "outputId": "ab7dc3aa-1335-4255-8ef9-ea0ce2c08307"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# RE-TRAIN WITH ALIGNED FEATURES (QUICK VERSION)\n",
        "# ======================\n",
        "\n",
        "print(\"üîÑ Training model with aligned features...\")\n",
        "\n",
        "# Create sequences with aligned features\n",
        "X_seq_aligned, y_seq_aligned = create_sequences(X_train_scaled_aligned, y_train.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq_aligned))\n",
        "X_train_aligned, X_val_aligned = X_seq_aligned[:split_idx], X_seq_aligned[split_idx:]\n",
        "y_train_aligned, y_val_aligned = y_seq_aligned[:split_idx], y_seq_aligned[split_idx:]\n",
        "\n",
        "print(f\"üìä Aligned sequences shape: {X_seq_aligned.shape}\")\n",
        "\n",
        "# Build model with correct input shape\n",
        "model_aligned = create_enhanced_model(X_train_aligned.shape[1:])\n",
        "model_aligned.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train quickly (fewer epochs since we already know good parameters)\n",
        "history_aligned = model_aligned.fit(\n",
        "    X_train_aligned, y_train_aligned,\n",
        "    validation_data=(X_val_aligned, y_val_aligned),\n",
        "    epochs=30,  # Fewer epochs for quick training\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_pred_aligned = model_aligned.predict(X_val_aligned, verbose=0)\n",
        "val_rmse_aligned = np.sqrt(mean_squared_error(y_val_aligned, val_pred_aligned))\n",
        "print(f\" Aligned Model RMSE: {val_rmse_aligned:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgGEfdIwXwL7",
        "outputId": "fa1071b2-8485-454b-b586-6a2bb629d932"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Training model with aligned features...\n",
            "üìä Aligned sequences shape: (30640, 36, 45)\n",
            "Epoch 1/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 181ms/step - loss: 83.3467 - mae: 83.8450 - val_loss: 76.8963 - val_mae: 77.3946 - learning_rate: 0.0020\n",
            "Epoch 2/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 197ms/step - loss: 51.1861 - mae: 51.6820 - val_loss: 48.7370 - val_mae: 49.2326 - learning_rate: 0.0020\n",
            "Epoch 3/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 185ms/step - loss: 35.2505 - mae: 35.7447 - val_loss: 49.4234 - val_mae: 49.9193 - learning_rate: 0.0020\n",
            "Epoch 4/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 191ms/step - loss: 31.2208 - mae: 31.7148 - val_loss: 45.0255 - val_mae: 45.5215 - learning_rate: 0.0020\n",
            "Epoch 5/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 179ms/step - loss: 28.2710 - mae: 28.7646 - val_loss: 47.4143 - val_mae: 47.9100 - learning_rate: 0.0020\n",
            "Epoch 6/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 183ms/step - loss: 26.2409 - mae: 26.7342 - val_loss: 45.3620 - val_mae: 45.8578 - learning_rate: 0.0020\n",
            "Epoch 7/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 188ms/step - loss: 24.0627 - mae: 24.5545 - val_loss: 46.7598 - val_mae: 47.2553 - learning_rate: 0.0020\n",
            "Epoch 8/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 191ms/step - loss: 22.7599 - mae: 23.2522 - val_loss: 46.0503 - val_mae: 46.5464 - learning_rate: 0.0020\n",
            "Epoch 9/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 205ms/step - loss: 21.8709 - mae: 22.3623 - val_loss: 48.3059 - val_mae: 48.8008 - learning_rate: 0.0020\n",
            "Epoch 10/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 183ms/step - loss: 20.8391 - mae: 21.3308 - val_loss: 48.6482 - val_mae: 49.1437 - learning_rate: 0.0020\n",
            "Epoch 11/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 179ms/step - loss: 19.9731 - mae: 20.4648 - val_loss: 48.1549 - val_mae: 48.6504 - learning_rate: 0.0020\n",
            "Epoch 12/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 19.2985 - mae: 19.7896\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 178ms/step - loss: 19.2977 - mae: 19.7887 - val_loss: 47.2275 - val_mae: 47.7218 - learning_rate: 0.0020\n",
            "Epoch 13/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 179ms/step - loss: 18.2896 - mae: 18.7789 - val_loss: 47.7764 - val_mae: 48.2717 - learning_rate: 0.0014\n",
            "Epoch 14/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 205ms/step - loss: 17.4492 - mae: 17.9396 - val_loss: 48.5195 - val_mae: 49.0148 - learning_rate: 0.0014\n",
            "Epoch 15/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 188ms/step - loss: 17.0729 - mae: 17.5628 - val_loss: 47.5775 - val_mae: 48.0725 - learning_rate: 0.0014\n",
            "Epoch 16/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 199ms/step - loss: 16.6922 - mae: 17.1821 - val_loss: 47.5271 - val_mae: 48.0224 - learning_rate: 0.0014\n",
            "Epoch 17/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 217ms/step - loss: 16.4676 - mae: 16.9568 - val_loss: 48.4707 - val_mae: 48.9653 - learning_rate: 0.0014\n",
            "Epoch 18/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 204ms/step - loss: 16.2338 - mae: 16.7231 - val_loss: 48.0253 - val_mae: 48.5196 - learning_rate: 0.0014\n",
            "Epoch 19/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 181ms/step - loss: 16.0610 - mae: 16.5505 - val_loss: 48.6598 - val_mae: 49.1553 - learning_rate: 0.0014\n",
            " Aligned Model RMSE: 74.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# GENERATE PREDICTIONS (WILL WORK NOW)\n",
        "# ======================\n",
        "\n",
        "print(\"üîÆ Generating test predictions with aligned features...\")\n",
        "\n",
        "test_sequences = []\n",
        "for i in range(len(X_test_scaled_aligned)):\n",
        "    if i < SEQUENCE_LENGTH:\n",
        "        needed_from_train = SEQUENCE_LENGTH - (i + 1)\n",
        "        if needed_from_train > 0:\n",
        "            sequence = np.vstack([X_train_scaled_aligned[-needed_from_train:], X_test_scaled_aligned[:i+1]])\n",
        "        else:\n",
        "            sequence = X_test_scaled_aligned[:SEQUENCE_LENGTH]\n",
        "    else:\n",
        "        sequence = X_test_scaled_aligned[i-SEQUENCE_LENGTH+1:i+1]\n",
        "\n",
        "    if sequence.shape[0] != SEQUENCE_LENGTH:\n",
        "        if sequence.shape[0] < SEQUENCE_LENGTH:\n",
        "            padding_needed = SEQUENCE_LENGTH - sequence.shape[0]\n",
        "            padding = np.repeat(sequence[0:1], padding_needed, axis=0)\n",
        "            sequence = np.vstack([padding, sequence])\n",
        "        else:\n",
        "            sequence = sequence[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    test_sequences.append(sequence)\n",
        "\n",
        "X_test_seq = np.array(test_sequences)\n",
        "print(f\"üìä Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "# This will work now - same feature dimensions!\n",
        "test_predictions = model_aligned.predict(X_test_seq, verbose=0)\n",
        "test_predictions = np.maximum(test_predictions.flatten(), 0)\n",
        "\n",
        "print(f\"üìä Predictions - Min: {test_predictions.min():.1f}, Max: {test_predictions.max():.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewGYEfVeXyWa",
        "outputId": "04410c2d-d352-4a55-d35a-beee6cd23024"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÆ Generating test predictions with aligned features...\n",
            "üìä Test sequences shape: (13148, 36, 45)\n",
            "üìä Predictions - Min: 6.0, Max: 355.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ Building and training LSTM model...\")\n",
        "\n",
        "# Create and train model\n",
        "model = create_enhanced_model(X_train.shape[1:])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_seq,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1_J-DptOGx2",
        "outputId": "f8b62bf8-8fd1-43d7-87c2-49c690583639"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Building and training LSTM model...\n",
            "Epoch 1/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 195ms/step - loss: 82.9128 - mae: 83.4111 - val_loss: 58.5081 - val_mae: 59.0051 - learning_rate: 0.0020\n",
            "Epoch 2/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 206ms/step - loss: 44.7753 - mae: 45.2711 - val_loss: 51.8344 - val_mae: 52.3311 - learning_rate: 0.0020\n",
            "Epoch 3/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 188ms/step - loss: 37.1888 - mae: 37.6836 - val_loss: 47.8030 - val_mae: 48.2966 - learning_rate: 0.0020\n",
            "Epoch 4/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - loss: 33.3223 - mae: 33.8163 - val_loss: 45.6550 - val_mae: 46.1499 - learning_rate: 0.0020\n",
            "Epoch 5/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 206ms/step - loss: 30.5232 - mae: 31.0170 - val_loss: 47.7789 - val_mae: 48.2737 - learning_rate: 0.0020\n",
            "Epoch 6/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 184ms/step - loss: 28.7558 - mae: 29.2500 - val_loss: 46.0650 - val_mae: 46.5598 - learning_rate: 0.0020\n",
            "Epoch 7/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 202ms/step - loss: 26.5616 - mae: 27.0549 - val_loss: 45.6625 - val_mae: 46.1588 - learning_rate: 0.0020\n",
            "Epoch 8/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 180ms/step - loss: 24.7060 - mae: 25.1987 - val_loss: 46.5441 - val_mae: 47.0399 - learning_rate: 0.0020\n",
            "Epoch 9/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 200ms/step - loss: 23.3387 - mae: 23.8305 - val_loss: 46.6396 - val_mae: 47.1352 - learning_rate: 0.0020\n",
            "Epoch 10/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 178ms/step - loss: 22.0664 - mae: 22.5585 - val_loss: 47.0664 - val_mae: 47.5612 - learning_rate: 0.0020\n",
            "Epoch 11/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 173ms/step - loss: 21.2582 - mae: 21.7499 - val_loss: 48.7263 - val_mae: 49.2221 - learning_rate: 0.0020\n",
            "Epoch 12/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - loss: 20.3921 - mae: 20.8840\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 194ms/step - loss: 20.3901 - mae: 20.8821 - val_loss: 48.3256 - val_mae: 48.8196 - learning_rate: 0.0020\n",
            "Epoch 13/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 189ms/step - loss: 19.1157 - mae: 19.6068 - val_loss: 48.6218 - val_mae: 49.1159 - learning_rate: 0.0014\n",
            "Epoch 14/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 176ms/step - loss: 18.3148 - mae: 18.8059 - val_loss: 48.9101 - val_mae: 49.4039 - learning_rate: 0.0014\n",
            "Epoch 15/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 194ms/step - loss: 17.8806 - mae: 18.3714 - val_loss: 49.2586 - val_mae: 49.7541 - learning_rate: 0.0014\n",
            "Epoch 16/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 190ms/step - loss: 17.4520 - mae: 17.9427 - val_loss: 46.5412 - val_mae: 47.0363 - learning_rate: 0.0014\n",
            "Epoch 17/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 193ms/step - loss: 16.7793 - mae: 17.2702 - val_loss: 49.6415 - val_mae: 50.1362 - learning_rate: 0.0014\n",
            "Epoch 18/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 208ms/step - loss: 16.7048 - mae: 17.1948 - val_loss: 49.1552 - val_mae: 49.6501 - learning_rate: 0.0014\n",
            "Epoch 19/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 177ms/step - loss: 16.7590 - mae: 17.2493 - val_loss: 49.3169 - val_mae: 49.8115 - learning_rate: 0.0014\n",
            "‚úÖ Model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä Evaluating model performance...\")\n",
        "\n",
        "# Evaluate model\n",
        "val_pred = model.predict(X_val, verbose=0)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "print(f\"\\nüéØ Model Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Target: < 3000 RMSE\")\n",
        "\n",
        "if val_rmse < 3000:\n",
        "    print(\"üéâ TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"üìà Need {val_rmse - 3000:.1f} points improvement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr6P3vWfHlCH",
        "outputId": "d69ed0e9-ab7f-45f8-f367-dceba3b03d89"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating model performance...\n",
            "\n",
            "üéØ Model Performance:\n",
            "Validation RMSE: 75.05\n",
            "Target: < 3000 RMSE\n",
            "üéâ TARGET ACHIEVED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CORRECT FEATURE ALIGNMENT\n",
        "# ======================\n",
        "\n",
        "print(\"üîß Correcting feature alignment...\")\n",
        "\n",
        "# First, let's see what features we actually have\n",
        "print(\"üìä Training features:\", train_enhanced.columns.tolist())\n",
        "print(\"üìä Test features:\", test_enhanced.columns.tolist())\n",
        "\n",
        "# The problem: Training has PM2.5 lag features that test can't have\n",
        "# Solution: Remove PM2.5-dependent features from training\n",
        "pm25_dependent_features = [col for col in train_enhanced.columns if 'pm2.5_' in col]\n",
        "print(f\"üö´ Removing PM2.5-dependent features: {pm25_dependent_features}\")\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "print(f\"‚úÖ Using {len(common_features)} common features:\")\n",
        "print(common_features)\n",
        "\n",
        "# Create aligned datasets\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "print(f\"üìä Aligned training features: {X_train_aligned.shape}\")\n",
        "print(f\"üìä Aligned test features: {X_test_aligned.shape}\")\n",
        "\n",
        "# Scale the aligned features\n",
        "scaler_aligned = StandardScaler()\n",
        "X_train_scaled_aligned = scaler_aligned.fit_transform(X_train_aligned)\n",
        "X_test_scaled_aligned = scaler_aligned.transform(X_test_aligned)\n",
        "\n",
        "print(\"‚úÖ Feature alignment completed!\")"
      ],
      "metadata": {
        "id": "0SDDX2L1VF_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447749f2-d1aa-4172-e7a4-51c37dd392a7"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Correcting feature alignment...\n",
            "üìä Training features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üìä Test features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üö´ Removing PM2.5-dependent features: ['pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48']\n",
            "‚úÖ Using 45 common features:\n",
            "['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üìä Aligned training features: (30676, 45)\n",
            "üìä Aligned test features: (13148, 45)\n",
            "‚úÖ Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CREATE SUBMISSION\n",
        "# ======================\n",
        "\n",
        "print(\"üíæ Creating submission file...\")\n",
        "\n",
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ EXPERIMENT COMPLETE!\")\n",
        "print(f\"üìà Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"üíæ Submission saved: {filename}\")\n",
        "print(f\"\\nüìã Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"‚úÖ Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"üìÅ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-U4WGAH1sE",
        "outputId": "0773edc4-1139-45c3-a7f8-924355bb9367"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Creating submission file...\n",
            "‚úÖ Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "üìä Predictions - Min: 6.0, Max: 355.0\n",
            "\n",
            "üéØ EXPERIMENT COMPLETE!\n",
            "üìà Validation RMSE: 75.05\n",
            "üíæ Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "\n",
            "üìã Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0  2013-07-02 04:00:00     17\n",
            "1  2013-07-02 05:00:00     16\n",
            "2  2013-07-02 06:00:00     16\n",
            "3  2013-07-02 07:00:00     16\n",
            "4  2013-07-02 08:00:00     16\n",
            "5  2013-07-02 09:00:00     16\n",
            "6  2013-07-02 10:00:00     15\n",
            "7  2013-07-02 11:00:00     15\n",
            "8  2013-07-02 12:00:00     15\n",
            "9  2013-07-02 13:00:00     16\n",
            "‚úÖ Confirmed: submissions/20250920_104754_enhanced_lstm_rmse_75.csv exists in submissions folder!\n",
            "üìÅ File size: 305933 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ EXPERIMENT COMPLETE!\")\n",
        "print(f\"üìà Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"üíæ Submission saved: {filename}\")\n",
        "print(f\"\\nüìã Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"‚úÖ Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"üìÅ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_YV-N-gJBAv",
        "outputId": "75537d80-701c-4788-d099-f034606aebd1"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "üìä Predictions - Min: 6.0, Max: 355.0\n",
            "\n",
            "üéØ EXPERIMENT COMPLETE!\n",
            "üìà Validation RMSE: 75.05\n",
            "üíæ Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "\n",
            "üìã Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0  2013-07-02 04:00:00     17\n",
            "1  2013-07-02 05:00:00     16\n",
            "2  2013-07-02 06:00:00     16\n",
            "3  2013-07-02 07:00:00     16\n",
            "4  2013-07-02 08:00:00     16\n",
            "5  2013-07-02 09:00:00     16\n",
            "6  2013-07-02 10:00:00     15\n",
            "7  2013-07-02 11:00:00     15\n",
            "8  2013-07-02 12:00:00     15\n",
            "9  2013-07-02 13:00:00     16\n",
            "‚úÖ Confirmed: submissions/20250920_104754_enhanced_lstm_rmse_75.csv exists in submissions folder!\n",
            "üìÅ File size: 305933 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(test_predictions, experiment_name, test.index)\n",
        "\n",
        "# DOWNLOAD THE SUBMISSION FILE\n",
        "download_submission(filename)\n",
        "\n",
        "print(f\"\\nüéØ EXPERIMENT COMPLETE!\")\n",
        "print(f\"üìà Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"üíæ Submission ready for Kaggle: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "G0iTXqeWjCyk",
        "outputId": "bcdd98ef-031c-4422-ebf9-ebe8684d035b"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Submission saved: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "üìä Predictions - Min: 6.0, Max: 355.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_404d6766-8db3-42b1-ab62-cfc30ab52e59\", \"20250920_104754_enhanced_lstm_rmse_75.csv\", 305933)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Download initiated: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n",
            "\n",
            "üéØ EXPERIMENT COMPLETE!\n",
            "üìà Validation RMSE: 75.05\n",
            "üíæ Submission ready for Kaggle: submissions/20250920_104754_enhanced_lstm_rmse_75.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data - Use the aligned features that match your training\n",
        "X_test = test_enhanced[common_features]  # Use the common features from alignment\n",
        "\n",
        "# Scale the test data using the same scaler you used for training\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create sequences for test data (same method as used in training)\n",
        "test_sequences = []\n",
        "for i in range(len(X_test_scaled)):\n",
        "    if i < SEQUENCE_LENGTH:\n",
        "        needed_from_train = SEQUENCE_LENGTH - (i + 1)\n",
        "        if needed_from_train > 0:\n",
        "            sequence = np.vstack([X_train_scaled[-needed_from_train:], X_test_scaled[:i+1]])\n",
        "        else:\n",
        "            sequence = X_test_scaled[:SEQUENCE_LENGTH]\n",
        "    else:\n",
        "        sequence = X_test_scaled[i-SEQUENCE_LENGTH+1:i+1]\n",
        "\n",
        "    if sequence.shape[0] != SEQUENCE_LENGTH:\n",
        "        if sequence.shape[0] < SEQUENCE_LENGTH:\n",
        "            padding_needed = SEQUENCE_LENGTH - sequence.shape[0]\n",
        "            padding = np.repeat(sequence[0:1], padding_needed, axis=0)\n",
        "            sequence = np.vstack([padding, sequence])\n",
        "        else:\n",
        "            sequence = sequence[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    test_sequences.append(sequence)\n",
        "\n",
        "X_test_seq = np.array(test_sequences)\n",
        "\n",
        "# Make predictions on the test set using the aligned model\n",
        "predictions = model_aligned.predict(X_test_seq, verbose=0)\n",
        "\n",
        "# Ensure predictions do not contain NaN values and are non-negative\n",
        "predictions = np.nan_to_num(predictions)\n",
        "predictions = np.maximum(predictions.flatten(), 0)\n",
        "\n",
        "# Convert predictions to integers\n",
        "predictions = np.round(predictions).astype(int)\n",
        "\n",
        "# Prepare the submission file - Use the exact same format as your working submission\n",
        "# Get the row IDs from your test index (already properly formatted in your working code)\n",
        "row_ids = test.index.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'row ID': row_ids,  # Use the same formatting that worked before\n",
        "    'pm2.5': predictions\n",
        "})\n",
        "\n",
        "# Save the file in CSV format for submission on Kaggle\n",
        "submission.to_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/subm_fixed.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Submission saved with {len(predictions)} predictions\")\n",
        "print(f\"üìä Predictions range: {predictions.min()} to {predictions.max()}\")\n",
        "print(f\"üìã Sample row IDs: {row_ids[:3].tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xj1KLZauUK8",
        "outputId": "ab3e7a9c-0d28-40c5-e378-268303b771a1"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Submission saved with 13148 predictions\n",
            "üìä Predictions range: 6 to 355\n",
            "üìã Sample row IDs: ['2013-07-02 04:00:00', '2013-07-02 05:00:00', '2013-07-02 06:00:00']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HW5Oi1kiel57"
      }
    }
  ]
}