{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkeza1/time_series_forecasting/blob/main/air_quality_forecasting_starter_code_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beijing Air Quality Forecasting Starter Notebook"
      ],
      "metadata": {
        "id": "iTsEYdtov6tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "nWkSHhqXrCqF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.style.use('default')\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "def save_submission(predictions, experiment_name, test_index):\n",
        "    \"\"\"Save submission with timestamp and experiment info\"\"\"\n",
        "    os.makedirs('submissions', exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': test_index.strftime('%Y-%m-%d %-H:%M:%S'),\n",
        "        'pm2.5': predictions.round().astype(int)\n",
        "    })\n",
        "\n",
        "    filename = f'submissions/{timestamp}_{experiment_name}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\" Submission saved: {filename}\")\n",
        "    print(f\"üìäPredictions - Min: {predictions.min():.1f}, Max: {predictions.max():.1f}\")\n",
        "\n",
        "    return filename, submission\n",
        "\n",
        "print(\"Submission tracking ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckzBsJN_FRc1",
        "outputId": "374a68ed-f0b8-410b-a995-687fd0d3c8b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "Submission tracking ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_C4HV99rHd5",
        "outputId": "003ee205-a6e6-4654-939b-259ad30c839d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "# Ensure train.csv and test.csv are saved in your Google Drive in the same folder.\n",
        "# Replace the file paths below with the actual paths to your dataset.\n",
        "train = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/test.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/sample_submission.csv')\n"
      ],
      "metadata": {
        "id": "gxW-6b_jrLAL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"üìä Training data: {train.shape}\")\n",
        "print(f\"üìä Test data: {test.shape}\")\n",
        "print(f\"\\nüìã Columns: {list(train.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjYBx9PmFuUA",
        "outputId": "d5f146e1-c0aa-49e2-c093-d07a933651cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Training data: (30676, 12)\n",
            "üìä Test data: (13148, 11)\n",
            "\n",
            "üìã Columns: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "print(train.head())\n",
        "print(test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCaTq55zJp00",
        "outputId": "3ad4aece-5f75-4fd4-fc86-9197ff5b45ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0   1 -1.580878 -1.922250  0.443328 -0.441894 -0.069353 -0.137667   \n",
            "1   2 -1.580878 -2.004228  0.345943 -0.379306 -0.069353 -0.137667   \n",
            "2   3 -1.580878 -1.922250  0.248559 -0.343514 -0.069353 -0.137667   \n",
            "3   4 -1.580878 -2.168183  0.248559 -0.280926 -0.069353 -0.137667   \n",
            "4   5 -1.511594 -2.004228  0.151174 -0.218339 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  pm2.5  \n",
            "0  2010-01-01 00:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "1  2010-01-01 01:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "2  2010-01-01 02:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "3  2010-01-01 03:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "4  2010-01-01 04:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "      No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0  30677  1.190496  0.701029 -2.186052 -0.003982 -0.069353 -0.137667   \n",
            "1  30678  1.121211  0.619051 -2.186052  0.031811 -0.069353 -0.137667   \n",
            "2  30679  1.190496  0.783006 -2.186052  0.094398 -0.069353 -0.137667   \n",
            "3  30680  1.190496  0.946961 -2.088668  0.174782 -0.069353 -0.137667   \n",
            "4  30681  1.190496  1.192893 -1.991283  0.210575 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  \n",
            "0  2013-07-02 04:00:00  1.448138 -0.732019 -0.522096  \n",
            "1  2013-07-02 05:00:00  1.448138 -0.732019 -0.522096  \n",
            "2  2013-07-02 06:00:00  1.448138 -0.732019 -0.522096  \n",
            "3  2013-07-02 07:00:00  1.448138 -0.732019 -0.522096  \n",
            "4  2013-07-02 08:00:00  1.448138 -0.732019 -0.522096  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n‚ùì Missing values:\")\n",
        "print(f\"Train: {train.isnull().sum().sum()}\")\n",
        "print(f\"Test: {test.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXnJf-BDFzJn",
        "outputId": "8faa3a6c-88ad-4995-d306-0f602dbb35d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ùì Missing values:\n",
            "Train: 1921\n",
            "Test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "train.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)"
      ],
      "metadata": {
        "id": "GDeAIfM8F9sl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Time range - Train: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Time range - Test: {test.index.min()} to {test.index.max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIpUlu08GDKp",
        "outputId": "0bcb93e1-1c98-4905-9f94-2f130eb32197"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time range - Train: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Time range - Test: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Key insights\n",
        "if 'pm2.5' in train.columns:\n",
        "    print(f\"\\nPM2.5 statistics:\")\n",
        "    print(f\"Mean: {train['pm2.5'].mean():.1f}, Std: {train['pm2.5'].std():.1f}\")\n",
        "    print(f\"Min: {train['pm2.5'].min():.1f}, Max: {train['pm2.5'].max():.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDLXFukLGNcB",
        "outputId": "46b5997f-2ecb-4832-c8fa-138b21bf1505"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PM2.5 statistics:\n",
            "Mean: 100.8, Std: 93.1\n",
            "Min: 0.0, Max: 994.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the training data\n",
        "\n",
        "In this sections explore your dataset with appropiate statistics and visualisations to understand your better. Ensure that you explain output of every code cell and what it entails."
      ],
      "metadata": {
        "id": "cRse3uqRrft5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature engineering\n",
        "train_enhanced = create_advanced_features(train_clean)\n",
        "test_enhanced = create_test_features(test_clean)\n",
        "\n",
        "# Handle remaining NaNs\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)"
      ],
      "metadata": {
        "id": "-M4xffncY8ZT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = train.copy()\n",
        "# First fill with forward fill, then backward fill, then interpolate\n",
        "train_clean = train_clean.fillna(method='ffill', limit=12)  # Limit forward fill to 12 hours\n",
        "train_clean = train_clean.fillna(method='bfill', limit=12)  # Limit backward fill to 12 hours\n",
        "train_clean = train_clean.interpolate(method='time')"
      ],
      "metadata": {
        "id": "ke0d3XEZGeYh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"‚úÖ After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"üìä Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zplS87XlGhkM",
        "outputId": "68b09cf7-2fc0-4146-ecd7-10dbb6e6fdf2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ After treatment - Train: 12, Test: 0\n",
            "üìä Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Feature Engineering\n",
        "def create_advanced_features(df, target_col='pm2.5'):\n",
        "    \"\"\"Create lag features, rolling statistics, and temporal features\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "     # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    if target_col in df_enhanced.columns:\n",
        "        # Lag features for PM2.5\n",
        "        for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_lag_{lag}'] = df_enhanced[target_col].shift(lag)\n",
        "             # Rolling statistics for PM2.5\n",
        "        for window in [6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_roll_mean_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'pm2.5_roll_std_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).std()\n",
        "            df_enhanced[f'pm2.5_roll_min_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).min()\n",
        "            df_enhanced[f'pm2.5_roll_max_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).max()\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "        # Drop original temporal columns (keep encoded versions)\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features(df):\n",
        "    \"\"\"Create features for test data (without PM2.5 lag features)\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "\n",
        "    # Drop original temporal columns\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "print(\"üõ†Ô∏è Feature engineering functions defined.\")\n",
        "\n",
        "# Apply feature engineering\n",
        "print(\"üîß Creating advanced features...\")\n",
        "train_enhanced = create_advanced_features(train_clean)\n",
        "test_enhanced = create_test_features(test_clean)\n",
        "\n",
        "print(f\" Original features: {train_clean.shape[1]}\")\n",
        "print(f\" Enhanced train features: {train_enhanced.shape[1]}\")\n",
        "print(f\" Enhanced test features: {test_enhanced.shape[1]}\")\n",
        "print(f\" New features added: {train_enhanced.shape[1] - train_clean.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTYrOYymC7uT",
        "outputId": "a86e5391-d574-4fdc-aa38-16d32733a777"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Feature engineering functions defined.\n",
            "üîß Creating advanced features...\n",
            " Original features: 11\n",
            " Enhanced train features: 70\n",
            " Enhanced test features: 46\n",
            " New features added: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\" Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "\n",
        "print(f\"Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R74CEBFrYok",
        "outputId": "25e27d4b-8bb1-43db-b7a6-176fddd97c6a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Handling remaining NaN values...\n",
            "Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle missing values\n",
        "\n",
        "\n",
        "- Check the dataset for missing values and decide how to handle them.\n",
        "- In this example, missing values are filled with the mean. You can experiment with other strategies."
      ],
      "metadata": {
        "id": "ABAqt0Jztd5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "print(\"üîß Handling missing values...\")\n",
        "\n",
        "# For training data\n",
        "train_clean = train.copy()\n",
        "train_clean = train_clean.fillna(method='ffill')  # Forward fill\n",
        "train_clean = train_clean.fillna(method='bfill')  # Backward fill\n",
        "train_clean = train_clean.interpolate(method='linear')  # Linear interpolation\n",
        "\n",
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"‚úÖ After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"üìä Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "id": "u2n29Ge1tami",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3485a58-9274-41dc-dc30-dca4f9d9863e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Handling missing values...\n",
            "‚úÖ After treatment - Train: 0, Test: 0\n",
            "üìä Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\"üîß Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)\n",
        "\n",
        "print(f\"‚úÖ Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXE3GFYn6PsP",
        "outputId": "7992968e-c084-40e4-c388-3e09fe6899f2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Handling remaining NaN values...\n",
            "‚úÖ Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate features and target\n",
        "\n",
        "- Feel free to trop any non-essential columns like that you think might not contribute to modeling."
      ],
      "metadata": {
        "id": "YKndkdRuty1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time series sequence creation\n",
        "def create_sequences(data, target, sequence_length=24):\n",
        "    \"\"\"Create sequences for LSTM input\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "QETLRAo_tvQH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# SEPARATE FEATURES & TARGET\n",
        "# ======================\n",
        "\n",
        "print(\"üéØ Separating features and target variables...\")\n",
        "\n",
        "# For training data - use the common features we already identified\n",
        "train_features = X_train_common  # This already has the common features only\n",
        "train_target = train_enhanced['pm2.5']\n",
        "\n",
        "# For test data - use the common features\n",
        "test_features = X_test_common    # This already has the common features only\n",
        "\n",
        "print(f\"üìä Training features shape: {train_features.shape}\")\n",
        "print(f\"üìä Training target shape: {train_target.shape}\")\n",
        "print(f\"üìä Test features shape: {test_features.shape}\")\n",
        "\n",
        "# Display feature names\n",
        "print(f\"\\nüîç Feature columns: {list(train_features.columns)}\")\n",
        "print(f\"üéØ Target column: pm2.5\")"
      ],
      "metadata": {
        "id": "NyP2mDjruG9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8718817-1d38-4b58-d76c-e4b88150fce8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Separating features and target variables...\n",
            "üìä Training features shape: (30676, 45)\n",
            "üìä Training target shape: (30676,)\n",
            "üìä Test features shape: (13148, 45)\n",
            "\n",
            "üîç Feature columns: ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üéØ Target column: pm2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üèóÔ∏è Creating time series sequences...\")\n",
        "X_seq, y_seq = create_sequences(X_train_scaled, train_target.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq))\n",
        "X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train_seq, y_val = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "print(f\"üìä Sequences shape: {X_seq.shape}\")\n",
        "print(f\"üìä Train: {X_train.shape}, Validation: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "TwaTWg66KY1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefd4fae-becd-4007-f263-89b02041e3d8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèóÔ∏è Creating time series sequences...\n",
            "üìä Sequences shape: (30640, 36, 68)\n",
            "üìä Train: (26044, 36, 68), Validation: (4596, 36, 68)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model\n",
        "\n",
        "Below is a simple LSTM model. Your task is to experiment with different parameters like, numbers of layers, units, activation functions, and optimizers, etc to get the best performing model. Experiment with other optimizers (e.g., SGD) or hyperparameters to improve performance."
      ],
      "metadata": {
        "id": "d488782wuR2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CORRECT FEATURE ALIGNMENT\n",
        "# ======================\n",
        "\n",
        "print(\"üîß Correcting feature alignment...\")\n",
        "\n",
        "# First, let's see what features we actually have\n",
        "print(\"üìä Training features:\", train_enhanced.columns.tolist())\n",
        "print(\"üìä Test features:\", test_enhanced.columns.tolist())\n",
        "\n",
        "# The problem: Training has PM2.5 lag features that test can't have\n",
        "# Solution: Remove PM2.5-dependent features from training\n",
        "pm25_dependent_features = [col for col in train_enhanced.columns if 'pm2.5_' in col]\n",
        "print(f\"üö´ Removing PM2.5-dependent features: {pm25_dependent_features}\")\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "print(f\"‚úÖ Using {len(common_features)} common features:\")\n",
        "print(common_features)\n",
        "\n",
        "# Create aligned datasets\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "print(f\"üìä Aligned training features: {X_train_aligned.shape}\")\n",
        "print(f\"üìä Aligned test features: {X_test_aligned.shape}\")\n",
        "\n",
        "# Scale the aligned features\n",
        "scaler_aligned = StandardScaler()\n",
        "X_train_scaled_aligned = scaler_aligned.fit_transform(X_train_aligned)\n",
        "X_test_scaled_aligned = scaler_aligned.transform(X_test_aligned)\n",
        "\n",
        "print(\"‚úÖ Feature alignment completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OSKbFbmXh16",
        "outputId": "3b01a882-fa4a-482d-bfec-4c56be73c30c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Correcting feature alignment...\n",
            "üìä Training features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üìä Test features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üö´ Removing PM2.5-dependent features: ['pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48']\n",
            "‚úÖ Using 45 common features:\n",
            "['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üìä Aligned training features: (30676, 45)\n",
            "üìä Aligned test features: (13148, 45)\n",
            "‚úÖ Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# RE-TRAIN WITH ALIGNED FEATURES (QUICK VERSION)\n",
        "# ======================\n",
        "\n",
        "print(\"üîÑ Training model with aligned features...\")\n",
        "\n",
        "# Create sequences with aligned features\n",
        "X_seq_aligned, y_seq_aligned = create_sequences(X_train_scaled_aligned, y_train.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq_aligned))\n",
        "X_train_aligned, X_val_aligned = X_seq_aligned[:split_idx], X_seq_aligned[split_idx:]\n",
        "y_train_aligned, y_val_aligned = y_seq_aligned[:split_idx], y_seq_aligned[split_idx:]\n",
        "\n",
        "print(f\"üìä Aligned sequences shape: {X_seq_aligned.shape}\")\n",
        "\n",
        "# Build model with correct input shape\n",
        "model_aligned = create_enhanced_model(X_train_aligned.shape[1:])\n",
        "model_aligned.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train quickly (fewer epochs since we already know good parameters)\n",
        "history_aligned = model_aligned.fit(\n",
        "    X_train_aligned, y_train_aligned,\n",
        "    validation_data=(X_val_aligned, y_val_aligned),\n",
        "    epochs=30,  # Fewer epochs for quick training\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_pred_aligned = model_aligned.predict(X_val_aligned, verbose=0)\n",
        "val_rmse_aligned = np.sqrt(mean_squared_error(y_val_aligned, val_pred_aligned))\n",
        "print(f\"üéØ Aligned Model RMSE: {val_rmse_aligned:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgGEfdIwXwL7",
        "outputId": "4129db91-9022-4e19-cfa3-4b0605b2e66a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Training model with aligned features...\n",
            "üìä Aligned sequences shape: (30640, 36, 45)\n",
            "Epoch 1/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - loss: 82.6972 - mae: 83.1958 - val_loss: 77.0363 - val_mae: 77.5345 - learning_rate: 0.0020\n",
            "Epoch 2/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - loss: 54.2871 - mae: 54.7843 - val_loss: 49.8468 - val_mae: 50.3418 - learning_rate: 0.0020\n",
            "Epoch 3/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - loss: 35.9352 - mae: 36.4298 - val_loss: 44.0857 - val_mae: 44.5804 - learning_rate: 0.0020\n",
            "Epoch 4/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 193ms/step - loss: 31.1083 - mae: 31.6023 - val_loss: 41.7749 - val_mae: 42.2707 - learning_rate: 0.0020\n",
            "Epoch 5/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - loss: 28.5551 - mae: 29.0485 - val_loss: 42.3813 - val_mae: 42.8761 - learning_rate: 0.0020\n",
            "Epoch 6/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 209ms/step - loss: 26.1680 - mae: 26.6614 - val_loss: 41.6764 - val_mae: 42.1718 - learning_rate: 0.0020\n",
            "Epoch 7/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 194ms/step - loss: 24.4072 - mae: 24.9004 - val_loss: 43.1282 - val_mae: 43.6235 - learning_rate: 0.0020\n",
            "Epoch 8/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 197ms/step - loss: 23.2828 - mae: 23.7753 - val_loss: 42.9285 - val_mae: 43.4241 - learning_rate: 0.0020\n",
            "Epoch 9/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 182ms/step - loss: 21.9319 - mae: 22.4244 - val_loss: 42.5014 - val_mae: 42.9968 - learning_rate: 0.0020\n",
            "Epoch 10/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 176ms/step - loss: 21.0703 - mae: 21.5623 - val_loss: 45.1492 - val_mae: 45.6430 - learning_rate: 0.0020\n",
            "Epoch 11/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 175ms/step - loss: 20.2414 - mae: 20.7337 - val_loss: 44.7816 - val_mae: 45.2763 - learning_rate: 0.0020\n",
            "Epoch 12/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 176ms/step - loss: 19.6158 - mae: 20.1071 - val_loss: 44.5042 - val_mae: 44.9992 - learning_rate: 0.0020\n",
            "Epoch 13/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 191ms/step - loss: 18.6669 - mae: 19.1581 - val_loss: 43.7093 - val_mae: 44.2034 - learning_rate: 0.0020\n",
            "Epoch 14/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 18.5158 - mae: 19.0074\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 179ms/step - loss: 18.5143 - mae: 19.0059 - val_loss: 44.9252 - val_mae: 45.4190 - learning_rate: 0.0020\n",
            "Epoch 15/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 174ms/step - loss: 17.3295 - mae: 17.8213 - val_loss: 45.2122 - val_mae: 45.7080 - learning_rate: 0.0014\n",
            "Epoch 16/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 177ms/step - loss: 16.7649 - mae: 17.2561 - val_loss: 45.8615 - val_mae: 46.3569 - learning_rate: 0.0014\n",
            "Epoch 17/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 187ms/step - loss: 16.6488 - mae: 17.1397 - val_loss: 45.9317 - val_mae: 46.4268 - learning_rate: 0.0014\n",
            "Epoch 18/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 188ms/step - loss: 16.1643 - mae: 16.6540 - val_loss: 46.1873 - val_mae: 46.6814 - learning_rate: 0.0014\n",
            "Epoch 19/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 198ms/step - loss: 15.9190 - mae: 16.4092 - val_loss: 45.8069 - val_mae: 46.3012 - learning_rate: 0.0014\n",
            "Epoch 20/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 172ms/step - loss: 15.5006 - mae: 15.9905 - val_loss: 46.4257 - val_mae: 46.9207 - learning_rate: 0.0014\n",
            "Epoch 21/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 183ms/step - loss: 15.0787 - mae: 15.5677 - val_loss: 46.4190 - val_mae: 46.9139 - learning_rate: 0.0014\n",
            "üéØ Aligned Model RMSE: 67.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# GENERATE PREDICTIONS (WILL WORK NOW)\n",
        "# ======================\n",
        "\n",
        "print(\"üîÆ Generating test predictions with aligned features...\")\n",
        "\n",
        "test_sequences = []\n",
        "for i in range(len(X_test_scaled_aligned)):\n",
        "    if i < SEQUENCE_LENGTH:\n",
        "        needed_from_train = SEQUENCE_LENGTH - (i + 1)\n",
        "        if needed_from_train > 0:\n",
        "            sequence = np.vstack([X_train_scaled_aligned[-needed_from_train:], X_test_scaled_aligned[:i+1]])\n",
        "        else:\n",
        "            sequence = X_test_scaled_aligned[:SEQUENCE_LENGTH]\n",
        "    else:\n",
        "        sequence = X_test_scaled_aligned[i-SEQUENCE_LENGTH+1:i+1]\n",
        "\n",
        "    if sequence.shape[0] != SEQUENCE_LENGTH:\n",
        "        if sequence.shape[0] < SEQUENCE_LENGTH:\n",
        "            padding_needed = SEQUENCE_LENGTH - sequence.shape[0]\n",
        "            padding = np.repeat(sequence[0:1], padding_needed, axis=0)\n",
        "            sequence = np.vstack([padding, sequence])\n",
        "        else:\n",
        "            sequence = sequence[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    test_sequences.append(sequence)\n",
        "\n",
        "X_test_seq = np.array(test_sequences)\n",
        "print(f\"üìä Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "# This will work now - same feature dimensions!\n",
        "test_predictions = model_aligned.predict(X_test_seq, verbose=0)\n",
        "test_predictions = np.maximum(test_predictions.flatten(), 0)\n",
        "\n",
        "print(f\"üìä Predictions - Min: {test_predictions.min():.1f}, Max: {test_predictions.max():.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewGYEfVeXyWa",
        "outputId": "558e9684-f8c5-40f2-ef8f-d0cddc8b3938"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÆ Generating test predictions with aligned features...\n",
            "üìä Test sequences shape: (13148, 36, 45)\n",
            "üìä Predictions - Min: 13.8, Max: 392.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ Building and training LSTM model...\")\n",
        "\n",
        "# Create and train model\n",
        "model = create_enhanced_model(X_train.shape[1:])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_seq,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1_J-DptOGx2",
        "outputId": "d765b03f-8193-424b-b075-843b84971d32"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Building and training LSTM model...\n",
            "Epoch 1/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 209ms/step - loss: 79.6651 - mae: 80.1634 - val_loss: 36.5207 - val_mae: 37.0150 - learning_rate: 0.0020\n",
            "Epoch 2/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 189ms/step - loss: 26.0483 - mae: 26.5417 - val_loss: 23.7085 - val_mae: 24.2011 - learning_rate: 0.0020\n",
            "Epoch 3/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 194ms/step - loss: 21.5637 - mae: 22.0560 - val_loss: 21.6454 - val_mae: 22.1370 - learning_rate: 0.0020\n",
            "Epoch 4/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 182ms/step - loss: 19.7368 - mae: 20.2289 - val_loss: 21.6713 - val_mae: 22.1614 - learning_rate: 0.0020\n",
            "Epoch 5/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - loss: 18.8892 - mae: 19.3802 - val_loss: 21.3461 - val_mae: 21.8367 - learning_rate: 0.0020\n",
            "Epoch 6/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 200ms/step - loss: 18.1183 - mae: 18.6095 - val_loss: 22.9608 - val_mae: 23.4525 - learning_rate: 0.0020\n",
            "Epoch 7/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 189ms/step - loss: 17.7294 - mae: 18.2206 - val_loss: 20.6523 - val_mae: 21.1437 - learning_rate: 0.0020\n",
            "Epoch 8/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 186ms/step - loss: 17.1730 - mae: 17.6634 - val_loss: 20.6981 - val_mae: 21.1899 - learning_rate: 0.0020\n",
            "Epoch 9/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 177ms/step - loss: 17.1246 - mae: 17.6147 - val_loss: 20.6073 - val_mae: 21.0985 - learning_rate: 0.0020\n",
            "Epoch 10/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 176ms/step - loss: 16.4996 - mae: 16.9898 - val_loss: 19.9541 - val_mae: 20.4448 - learning_rate: 0.0020\n",
            "Epoch 11/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 195ms/step - loss: 15.9852 - mae: 16.4743 - val_loss: 20.2294 - val_mae: 20.7207 - learning_rate: 0.0020\n",
            "Epoch 12/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 184ms/step - loss: 15.7902 - mae: 16.2788 - val_loss: 20.3119 - val_mae: 20.8040 - learning_rate: 0.0020\n",
            "Epoch 13/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 187ms/step - loss: 15.2533 - mae: 15.7419 - val_loss: 20.1009 - val_mae: 20.5923 - learning_rate: 0.0020\n",
            "Epoch 14/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 184ms/step - loss: 14.8443 - mae: 15.3320 - val_loss: 20.4927 - val_mae: 20.9843 - learning_rate: 0.0020\n",
            "Epoch 15/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - loss: 14.8740 - mae: 15.3626 - val_loss: 20.4455 - val_mae: 20.9368 - learning_rate: 0.0020\n",
            "Epoch 16/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 207ms/step - loss: 14.7112 - mae: 15.2001 - val_loss: 20.6145 - val_mae: 21.1051 - learning_rate: 0.0020\n",
            "Epoch 17/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 201ms/step - loss: 14.4283 - mae: 14.9172 - val_loss: 21.2159 - val_mae: 21.7075 - learning_rate: 0.0020\n",
            "Epoch 18/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 14.1564 - mae: 14.6445\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 193ms/step - loss: 14.1558 - mae: 14.6438 - val_loss: 20.8572 - val_mae: 21.3488 - learning_rate: 0.0020\n",
            "Epoch 19/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 303ms/step - loss: 13.8052 - mae: 14.2936 - val_loss: 20.7622 - val_mae: 21.2520 - learning_rate: 0.0014\n",
            "Epoch 20/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 255ms/step - loss: 13.5092 - mae: 13.9977 - val_loss: 21.2491 - val_mae: 21.7403 - learning_rate: 0.0014\n",
            "Epoch 21/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 206ms/step - loss: 13.2313 - mae: 13.7192 - val_loss: 20.9978 - val_mae: 21.4896 - learning_rate: 0.0014\n",
            "Epoch 22/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 201ms/step - loss: 13.0059 - mae: 13.4931 - val_loss: 21.9784 - val_mae: 22.4684 - learning_rate: 0.0014\n",
            "Epoch 23/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 204ms/step - loss: 13.2277 - mae: 13.7145 - val_loss: 21.6501 - val_mae: 22.1414 - learning_rate: 0.0014\n",
            "Epoch 24/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 194ms/step - loss: 12.9408 - mae: 13.4277 - val_loss: 21.9551 - val_mae: 22.4479 - learning_rate: 0.0014\n",
            "Epoch 25/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 211ms/step - loss: 12.6803 - mae: 13.1668 - val_loss: 21.9211 - val_mae: 22.4129 - learning_rate: 0.0014\n",
            "‚úÖ Model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìä Evaluating model performance...\")\n",
        "\n",
        "# Evaluate model\n",
        "val_pred = model.predict(X_val, verbose=0)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "print(f\"\\nüéØ Model Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Target: < 3000 RMSE\")\n",
        "\n",
        "if val_rmse < 3000:\n",
        "    print(\"üéâ TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"üìà Need {val_rmse - 3000:.1f} points improvement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr6P3vWfHlCH",
        "outputId": "eada43c7-f740-4f5e-a1d5-4a3500397c64"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Evaluating model performance...\n",
            "\n",
            "üéØ Model Performance:\n",
            "Validation RMSE: 37.26\n",
            "Target: < 3000 RMSE\n",
            "üéâ TARGET ACHIEVED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CORRECT FEATURE ALIGNMENT\n",
        "# ======================\n",
        "\n",
        "print(\"üîß Correcting feature alignment...\")\n",
        "\n",
        "# First, let's see what features we actually have\n",
        "print(\"üìä Training features:\", train_enhanced.columns.tolist())\n",
        "print(\"üìä Test features:\", test_enhanced.columns.tolist())\n",
        "\n",
        "# The problem: Training has PM2.5 lag features that test can't have\n",
        "# Solution: Remove PM2.5-dependent features from training\n",
        "pm25_dependent_features = [col for col in train_enhanced.columns if 'pm2.5_' in col]\n",
        "print(f\"üö´ Removing PM2.5-dependent features: {pm25_dependent_features}\")\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "print(f\"‚úÖ Using {len(common_features)} common features:\")\n",
        "print(common_features)\n",
        "\n",
        "# Create aligned datasets\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "print(f\"üìä Aligned training features: {X_train_aligned.shape}\")\n",
        "print(f\"üìä Aligned test features: {X_test_aligned.shape}\")\n",
        "\n",
        "# Scale the aligned features\n",
        "scaler_aligned = StandardScaler()\n",
        "X_train_scaled_aligned = scaler_aligned.fit_transform(X_train_aligned)\n",
        "X_test_scaled_aligned = scaler_aligned.transform(X_test_aligned)\n",
        "\n",
        "print(\"‚úÖ Feature alignment completed!\")"
      ],
      "metadata": {
        "id": "0SDDX2L1VF_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c1962e-4851-4b1a-fadf-e02fd81e3fe6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Correcting feature alignment...\n",
            "üìä Training features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üìä Test features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üö´ Removing PM2.5-dependent features: ['pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48']\n",
            "‚úÖ Using 45 common features:\n",
            "['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "üìä Aligned training features: (30676, 45)\n",
            "üìä Aligned test features: (13148, 45)\n",
            "‚úÖ Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CREATE SUBMISSION\n",
        "# ======================\n",
        "\n",
        "print(\"üíæ Creating submission file...\")\n",
        "\n",
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ EXPERIMENT COMPLETE!\")\n",
        "print(f\"üìà Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"üíæ Submission saved: {filename}\")\n",
        "print(f\"\\nüìã Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"‚úÖ Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"üìÅ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-U4WGAH1sE",
        "outputId": "ae9dbc33-a0a3-43ce-8407-72f10ab6b2c5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Creating submission file...\n",
            "‚úÖ Submission saved: submissions/20250920_092621_enhanced_lstm_rmse_37.csv\n",
            "üìä Predictions - Min: 13.8, Max: 392.3\n",
            "\n",
            "üéØ EXPERIMENT COMPLETE!\n",
            "üìà Validation RMSE: 37.26\n",
            "üíæ Submission saved: submissions/20250920_092621_enhanced_lstm_rmse_37.csv\n",
            "\n",
            "üìã Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0   2013-07-02 4:00:00     31\n",
            "1   2013-07-02 5:00:00     30\n",
            "2   2013-07-02 6:00:00     29\n",
            "3   2013-07-02 7:00:00     28\n",
            "4   2013-07-02 8:00:00     28\n",
            "5   2013-07-02 9:00:00     27\n",
            "6  2013-07-02 10:00:00     25\n",
            "7  2013-07-02 11:00:00     25\n",
            "8  2013-07-02 12:00:00     24\n",
            "9  2013-07-02 13:00:00     24\n",
            "‚úÖ Confirmed: submissions/20250920_092621_enhanced_lstm_rmse_37.csv exists in submissions folder!\n",
            "üìÅ File size: 301342 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nüéØ EXPERIMENT COMPLETE!\")\n",
        "print(f\"üìà Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"üíæ Submission saved: {filename}\")\n",
        "print(f\"\\nüìã Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"‚úÖ Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"üìÅ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"‚ùå Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_YV-N-gJBAv",
        "outputId": "01f03bf2-d7b2-449d-9e38-aa9cfc540e96"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Submission saved: submissions/20250920_092625_enhanced_lstm_rmse_37.csv\n",
            "üìä Predictions - Min: 13.8, Max: 392.3\n",
            "\n",
            "üéØ EXPERIMENT COMPLETE!\n",
            "üìà Validation RMSE: 37.26\n",
            "üíæ Submission saved: submissions/20250920_092625_enhanced_lstm_rmse_37.csv\n",
            "\n",
            "üìã Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0   2013-07-02 4:00:00     31\n",
            "1   2013-07-02 5:00:00     30\n",
            "2   2013-07-02 6:00:00     29\n",
            "3   2013-07-02 7:00:00     28\n",
            "4   2013-07-02 8:00:00     28\n",
            "5   2013-07-02 9:00:00     27\n",
            "6  2013-07-02 10:00:00     25\n",
            "7  2013-07-02 11:00:00     25\n",
            "8  2013-07-02 12:00:00     24\n",
            "9  2013-07-02 13:00:00     24\n",
            "‚úÖ Confirmed: submissions/20250920_092625_enhanced_lstm_rmse_37.csv exists in submissions folder!\n",
            "üìÅ File size: 301342 bytes\n"
          ]
        }
      ]
    }
  ]
}