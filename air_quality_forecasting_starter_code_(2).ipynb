{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkeza1/time_series_forecasting/blob/main/air_quality_forecasting_starter_code_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beijing Air Quality Forecasting Starter Notebook"
      ],
      "metadata": {
        "id": "iTsEYdtov6tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "nWkSHhqXrCqF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.style.use('default')\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "def save_submission(predictions, experiment_name, test_index):\n",
        "    \"\"\"Save submission with timestamp and experiment info\"\"\"\n",
        "    os.makedirs('submissions', exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': test_index.strftime('%Y-%m-%d %-H:%M:%S'),\n",
        "        'pm2.5': predictions.round().astype(int)\n",
        "    })\n",
        "\n",
        "    filename = f'submissions/{timestamp}_{experiment_name}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\" Submission saved: {filename}\")\n",
        "    print(f\"ğŸ“ŠPredictions - Min: {predictions.min():.1f}, Max: {predictions.max():.1f}\")\n",
        "\n",
        "    return filename, submission\n",
        "\n",
        "print(\"Submission tracking ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckzBsJN_FRc1",
        "outputId": "374a68ed-f0b8-410b-a995-687fd0d3c8b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "Submission tracking ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_C4HV99rHd5",
        "outputId": "003ee205-a6e6-4654-939b-259ad30c839d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "# Ensure train.csv and test.csv are saved in your Google Drive in the same folder.\n",
        "# Replace the file paths below with the actual paths to your dataset.\n",
        "train = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/test.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Kaggle_competition_ML/air_quality_forecasting/sample_submission.csv')\n"
      ],
      "metadata": {
        "id": "gxW-6b_jrLAL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ğŸ“Š Training data: {train.shape}\")\n",
        "print(f\"ğŸ“Š Test data: {test.shape}\")\n",
        "print(f\"\\nğŸ“‹ Columns: {list(train.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjYBx9PmFuUA",
        "outputId": "d5f146e1-c0aa-49e2-c093-d07a933651cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Training data: (30676, 12)\n",
            "ğŸ“Š Test data: (13148, 11)\n",
            "\n",
            "ğŸ“‹ Columns: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "print(train.head())\n",
        "print(test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCaTq55zJp00",
        "outputId": "3ad4aece-5f75-4fd4-fc86-9197ff5b45ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0   1 -1.580878 -1.922250  0.443328 -0.441894 -0.069353 -0.137667   \n",
            "1   2 -1.580878 -2.004228  0.345943 -0.379306 -0.069353 -0.137667   \n",
            "2   3 -1.580878 -1.922250  0.248559 -0.343514 -0.069353 -0.137667   \n",
            "3   4 -1.580878 -2.168183  0.248559 -0.280926 -0.069353 -0.137667   \n",
            "4   5 -1.511594 -2.004228  0.151174 -0.218339 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  pm2.5  \n",
            "0  2010-01-01 00:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "1  2010-01-01 01:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "2  2010-01-01 02:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "3  2010-01-01 03:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "4  2010-01-01 04:00:00  1.448138 -0.732019 -0.522096    NaN  \n",
            "      No      DEWP      TEMP      PRES       Iws        Is        Ir  \\\n",
            "0  30677  1.190496  0.701029 -2.186052 -0.003982 -0.069353 -0.137667   \n",
            "1  30678  1.121211  0.619051 -2.186052  0.031811 -0.069353 -0.137667   \n",
            "2  30679  1.190496  0.783006 -2.186052  0.094398 -0.069353 -0.137667   \n",
            "3  30680  1.190496  0.946961 -2.088668  0.174782 -0.069353 -0.137667   \n",
            "4  30681  1.190496  1.192893 -1.991283  0.210575 -0.069353 -0.137667   \n",
            "\n",
            "              datetime   cbwd_NW   cbwd_SE   cbwd_cv  \n",
            "0  2013-07-02 04:00:00  1.448138 -0.732019 -0.522096  \n",
            "1  2013-07-02 05:00:00  1.448138 -0.732019 -0.522096  \n",
            "2  2013-07-02 06:00:00  1.448138 -0.732019 -0.522096  \n",
            "3  2013-07-02 07:00:00  1.448138 -0.732019 -0.522096  \n",
            "4  2013-07-02 08:00:00  1.448138 -0.732019 -0.522096  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nâ“ Missing values:\")\n",
        "print(f\"Train: {train.isnull().sum().sum()}\")\n",
        "print(f\"Test: {test.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXnJf-BDFzJn",
        "outputId": "8faa3a6c-88ad-4995-d306-0f602dbb35d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â“ Missing values:\n",
            "Train: 1921\n",
            "Test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "train.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)"
      ],
      "metadata": {
        "id": "GDeAIfM8F9sl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Time range - Train: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"Time range - Test: {test.index.min()} to {test.index.max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIpUlu08GDKp",
        "outputId": "0bcb93e1-1c98-4905-9f94-2f130eb32197"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time range - Train: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "Time range - Test: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Key insights\n",
        "if 'pm2.5' in train.columns:\n",
        "    print(f\"\\nPM2.5 statistics:\")\n",
        "    print(f\"Mean: {train['pm2.5'].mean():.1f}, Std: {train['pm2.5'].std():.1f}\")\n",
        "    print(f\"Min: {train['pm2.5'].min():.1f}, Max: {train['pm2.5'].max():.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDLXFukLGNcB",
        "outputId": "46b5997f-2ecb-4832-c8fa-138b21bf1505"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PM2.5 statistics:\n",
            "Mean: 100.8, Std: 93.1\n",
            "Min: 0.0, Max: 994.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore the training data\n",
        "\n",
        "In this sections explore your dataset with appropiate statistics and visualisations to understand your better. Ensure that you explain output of every code cell and what it entails."
      ],
      "metadata": {
        "id": "cRse3uqRrft5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature engineering\n",
        "train_enhanced = create_advanced_features(train_clean)\n",
        "test_enhanced = create_test_features(test_clean)\n",
        "\n",
        "# Handle remaining NaNs\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)"
      ],
      "metadata": {
        "id": "-M4xffncY8ZT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = train.copy()\n",
        "# First fill with forward fill, then backward fill, then interpolate\n",
        "train_clean = train_clean.fillna(method='ffill', limit=12)  # Limit forward fill to 12 hours\n",
        "train_clean = train_clean.fillna(method='bfill', limit=12)  # Limit backward fill to 12 hours\n",
        "train_clean = train_clean.interpolate(method='time')"
      ],
      "metadata": {
        "id": "ke0d3XEZGeYh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"âœ… After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"ğŸ“Š Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zplS87XlGhkM",
        "outputId": "68b09cf7-2fc0-4146-ecd7-10dbb6e6fdf2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… After treatment - Train: 12, Test: 0\n",
            "ğŸ“Š Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Feature Engineering\n",
        "def create_advanced_features(df, target_col='pm2.5'):\n",
        "    \"\"\"Create lag features, rolling statistics, and temporal features\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "     # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    if target_col in df_enhanced.columns:\n",
        "        # Lag features for PM2.5\n",
        "        for lag in [1, 2, 3, 6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_lag_{lag}'] = df_enhanced[target_col].shift(lag)\n",
        "             # Rolling statistics for PM2.5\n",
        "        for window in [6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_roll_mean_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'pm2.5_roll_std_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).std()\n",
        "            df_enhanced[f'pm2.5_roll_min_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).min()\n",
        "            df_enhanced[f'pm2.5_roll_max_{window}'] = df_enhanced[target_col].rolling(window, min_periods=1).max()\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "        # Drop original temporal columns (keep encoded versions)\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features(df):\n",
        "    \"\"\"Create features for test data (without PM2.5 lag features)\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    df_enhanced['is_weekend'] = df_enhanced['day_of_week'].isin([5, 6]).astype(int)\n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "\n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    df_enhanced['humidity_index'] = df_enhanced['DEWP'] / (df_enhanced['TEMP'] + 1e-6)\n",
        "\n",
        "    # Rolling features for weather variables\n",
        "    for col in ['DEWP', 'TEMP', 'PRES', 'Iws']:\n",
        "        for window in [6, 12, 24]:\n",
        "            df_enhanced[f'{col}_roll_mean_{window}'] = df_enhanced[col].rolling(window, min_periods=1).mean()\n",
        "            df_enhanced[f'{col}_roll_std_{window}'] = df_enhanced[col].rolling(window, min_periods=1).std()\n",
        "\n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "\n",
        "    # Drop original temporal columns\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "\n",
        "    return df_enhanced\n",
        "\n",
        "print(\"ğŸ› ï¸ Feature engineering functions defined.\")\n",
        "\n",
        "# Apply feature engineering\n",
        "print(\"ğŸ”§ Creating advanced features...\")\n",
        "train_enhanced = create_advanced_features(train_clean)\n",
        "test_enhanced = create_test_features(test_clean)\n",
        "\n",
        "print(f\" Original features: {train_clean.shape[1]}\")\n",
        "print(f\" Enhanced train features: {train_enhanced.shape[1]}\")\n",
        "print(f\" Enhanced test features: {test_enhanced.shape[1]}\")\n",
        "print(f\" New features added: {train_enhanced.shape[1] - train_clean.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTYrOYymC7uT",
        "outputId": "a86e5391-d574-4fdc-aa38-16d32733a777"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ› ï¸ Feature engineering functions defined.\n",
            "ğŸ”§ Creating advanced features...\n",
            " Original features: 11\n",
            " Enhanced train features: 70\n",
            " Enhanced test features: 46\n",
            " New features added: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\" Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
        "\n",
        "print(f\"Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R74CEBFrYok",
        "outputId": "25e27d4b-8bb1-43db-b7a6-176fddd97c6a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Handling remaining NaN values...\n",
            "Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle missing values\n",
        "\n",
        "\n",
        "- Check the dataset for missing values and decide how to handle them.\n",
        "- In this example, missing values are filled with the mean. You can experiment with other strategies."
      ],
      "metadata": {
        "id": "ABAqt0Jztd5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "print(\"ğŸ”§ Handling missing values...\")\n",
        "\n",
        "# For training data\n",
        "train_clean = train.copy()\n",
        "train_clean = train_clean.fillna(method='ffill')  # Forward fill\n",
        "train_clean = train_clean.fillna(method='bfill')  # Backward fill\n",
        "train_clean = train_clean.interpolate(method='linear')  # Linear interpolation\n",
        "\n",
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"âœ… After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"ğŸ“Š Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")"
      ],
      "metadata": {
        "id": "u2n29Ge1tami",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3485a58-9274-41dc-dc30-dca4f9d9863e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Handling missing values...\n",
            "âœ… After treatment - Train: 0, Test: 0\n",
            "ğŸ“Š Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\"ğŸ”§ Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)\n",
        "\n",
        "print(f\"âœ… Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXE3GFYn6PsP",
        "outputId": "7992968e-c084-40e4-c388-3e09fe6899f2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Handling remaining NaN values...\n",
            "âœ… Final datasets - Train: (30676, 70), Test: (13148, 46)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate features and target\n",
        "\n",
        "- Feel free to trop any non-essential columns like that you think might not contribute to modeling."
      ],
      "metadata": {
        "id": "YKndkdRuty1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time series sequence creation\n",
        "def create_sequences(data, target, sequence_length=24):\n",
        "    \"\"\"Create sequences for LSTM input\"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "QETLRAo_tvQH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# SEPARATE FEATURES & TARGET\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ¯ Separating features and target variables...\")\n",
        "\n",
        "# For training data - use the common features we already identified\n",
        "train_features = X_train_common  # This already has the common features only\n",
        "train_target = train_enhanced['pm2.5']\n",
        "\n",
        "# For test data - use the common features\n",
        "test_features = X_test_common    # This already has the common features only\n",
        "\n",
        "print(f\"ğŸ“Š Training features shape: {train_features.shape}\")\n",
        "print(f\"ğŸ“Š Training target shape: {train_target.shape}\")\n",
        "print(f\"ğŸ“Š Test features shape: {test_features.shape}\")\n",
        "\n",
        "# Display feature names\n",
        "print(f\"\\nğŸ” Feature columns: {list(train_features.columns)}\")\n",
        "print(f\"ğŸ¯ Target column: pm2.5\")"
      ],
      "metadata": {
        "id": "NyP2mDjruG9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8718817-1d38-4b58-d76c-e4b88150fce8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Separating features and target variables...\n",
            "ğŸ“Š Training features shape: (30676, 45)\n",
            "ğŸ“Š Training target shape: (30676,)\n",
            "ğŸ“Š Test features shape: (13148, 45)\n",
            "\n",
            "ğŸ” Feature columns: ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ¯ Target column: pm2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ—ï¸ Creating time series sequences...\")\n",
        "X_seq, y_seq = create_sequences(X_train_scaled, train_target.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq))\n",
        "X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train_seq, y_val = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "print(f\"ğŸ“Š Sequences shape: {X_seq.shape}\")\n",
        "print(f\"ğŸ“Š Train: {X_train.shape}, Validation: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "TwaTWg66KY1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefd4fae-becd-4007-f263-89b02041e3d8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—ï¸ Creating time series sequences...\n",
            "ğŸ“Š Sequences shape: (30640, 36, 68)\n",
            "ğŸ“Š Train: (26044, 36, 68), Validation: (4596, 36, 68)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model\n",
        "\n",
        "Below is a simple LSTM model. Your task is to experiment with different parameters like, numbers of layers, units, activation functions, and optimizers, etc to get the best performing model. Experiment with other optimizers (e.g., SGD) or hyperparameters to improve performance."
      ],
      "metadata": {
        "id": "d488782wuR2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CORRECT FEATURE ALIGNMENT\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ”§ Correcting feature alignment...\")\n",
        "\n",
        "# First, let's see what features we actually have\n",
        "print(\"ğŸ“Š Training features:\", train_enhanced.columns.tolist())\n",
        "print(\"ğŸ“Š Test features:\", test_enhanced.columns.tolist())\n",
        "\n",
        "# The problem: Training has PM2.5 lag features that test can't have\n",
        "# Solution: Remove PM2.5-dependent features from training\n",
        "pm25_dependent_features = [col for col in train_enhanced.columns if 'pm2.5_' in col]\n",
        "print(f\"ğŸš« Removing PM2.5-dependent features: {pm25_dependent_features}\")\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "print(f\"âœ… Using {len(common_features)} common features:\")\n",
        "print(common_features)\n",
        "\n",
        "# Create aligned datasets\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "print(f\"ğŸ“Š Aligned training features: {X_train_aligned.shape}\")\n",
        "print(f\"ğŸ“Š Aligned test features: {X_test_aligned.shape}\")\n",
        "\n",
        "# Scale the aligned features\n",
        "scaler_aligned = StandardScaler()\n",
        "X_train_scaled_aligned = scaler_aligned.fit_transform(X_train_aligned)\n",
        "X_test_scaled_aligned = scaler_aligned.transform(X_test_aligned)\n",
        "\n",
        "print(\"âœ… Feature alignment completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OSKbFbmXh16",
        "outputId": "3b01a882-fa4a-482d-bfec-4c56be73c30c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Correcting feature alignment...\n",
            "ğŸ“Š Training features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ“Š Test features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸš« Removing PM2.5-dependent features: ['pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48']\n",
            "âœ… Using 45 common features:\n",
            "['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ“Š Aligned training features: (30676, 45)\n",
            "ğŸ“Š Aligned test features: (13148, 45)\n",
            "âœ… Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# RE-TRAIN WITH ALIGNED FEATURES (QUICK VERSION)\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ”„ Training model with aligned features...\")\n",
        "\n",
        "# Create sequences with aligned features\n",
        "X_seq_aligned, y_seq_aligned = create_sequences(X_train_scaled_aligned, y_train.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.85 * len(X_seq_aligned))\n",
        "X_train_aligned, X_val_aligned = X_seq_aligned[:split_idx], X_seq_aligned[split_idx:]\n",
        "y_train_aligned, y_val_aligned = y_seq_aligned[:split_idx], y_seq_aligned[split_idx:]\n",
        "\n",
        "print(f\"ğŸ“Š Aligned sequences shape: {X_seq_aligned.shape}\")\n",
        "\n",
        "# Build model with correct input shape\n",
        "model_aligned = create_enhanced_model(X_train_aligned.shape[1:])\n",
        "model_aligned.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Train quickly (fewer epochs since we already know good parameters)\n",
        "history_aligned = model_aligned.fit(\n",
        "    X_train_aligned, y_train_aligned,\n",
        "    validation_data=(X_val_aligned, y_val_aligned),\n",
        "    epochs=30,  # Fewer epochs for quick training\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_pred_aligned = model_aligned.predict(X_val_aligned, verbose=0)\n",
        "val_rmse_aligned = np.sqrt(mean_squared_error(y_val_aligned, val_pred_aligned))\n",
        "print(f\"ğŸ¯ Aligned Model RMSE: {val_rmse_aligned:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgGEfdIwXwL7",
        "outputId": "4129db91-9022-4e19-cfa3-4b0605b2e66a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Training model with aligned features...\n",
            "ğŸ“Š Aligned sequences shape: (30640, 36, 45)\n",
            "Epoch 1/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 187ms/step - loss: 82.6972 - mae: 83.1958 - val_loss: 77.0363 - val_mae: 77.5345 - learning_rate: 0.0020\n",
            "Epoch 2/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 178ms/step - loss: 54.2871 - mae: 54.7843 - val_loss: 49.8468 - val_mae: 50.3418 - learning_rate: 0.0020\n",
            "Epoch 3/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 177ms/step - loss: 35.9352 - mae: 36.4298 - val_loss: 44.0857 - val_mae: 44.5804 - learning_rate: 0.0020\n",
            "Epoch 4/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 193ms/step - loss: 31.1083 - mae: 31.6023 - val_loss: 41.7749 - val_mae: 42.2707 - learning_rate: 0.0020\n",
            "Epoch 5/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 181ms/step - loss: 28.5551 - mae: 29.0485 - val_loss: 42.3813 - val_mae: 42.8761 - learning_rate: 0.0020\n",
            "Epoch 6/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 209ms/step - loss: 26.1680 - mae: 26.6614 - val_loss: 41.6764 - val_mae: 42.1718 - learning_rate: 0.0020\n",
            "Epoch 7/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 194ms/step - loss: 24.4072 - mae: 24.9004 - val_loss: 43.1282 - val_mae: 43.6235 - learning_rate: 0.0020\n",
            "Epoch 8/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 197ms/step - loss: 23.2828 - mae: 23.7753 - val_loss: 42.9285 - val_mae: 43.4241 - learning_rate: 0.0020\n",
            "Epoch 9/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 182ms/step - loss: 21.9319 - mae: 22.4244 - val_loss: 42.5014 - val_mae: 42.9968 - learning_rate: 0.0020\n",
            "Epoch 10/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 176ms/step - loss: 21.0703 - mae: 21.5623 - val_loss: 45.1492 - val_mae: 45.6430 - learning_rate: 0.0020\n",
            "Epoch 11/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 175ms/step - loss: 20.2414 - mae: 20.7337 - val_loss: 44.7816 - val_mae: 45.2763 - learning_rate: 0.0020\n",
            "Epoch 12/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 176ms/step - loss: 19.6158 - mae: 20.1071 - val_loss: 44.5042 - val_mae: 44.9992 - learning_rate: 0.0020\n",
            "Epoch 13/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 191ms/step - loss: 18.6669 - mae: 19.1581 - val_loss: 43.7093 - val_mae: 44.2034 - learning_rate: 0.0020\n",
            "Epoch 14/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - loss: 18.5158 - mae: 19.0074\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 179ms/step - loss: 18.5143 - mae: 19.0059 - val_loss: 44.9252 - val_mae: 45.4190 - learning_rate: 0.0020\n",
            "Epoch 15/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 174ms/step - loss: 17.3295 - mae: 17.8213 - val_loss: 45.2122 - val_mae: 45.7080 - learning_rate: 0.0014\n",
            "Epoch 16/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 177ms/step - loss: 16.7649 - mae: 17.2561 - val_loss: 45.8615 - val_mae: 46.3569 - learning_rate: 0.0014\n",
            "Epoch 17/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 187ms/step - loss: 16.6488 - mae: 17.1397 - val_loss: 45.9317 - val_mae: 46.4268 - learning_rate: 0.0014\n",
            "Epoch 18/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 188ms/step - loss: 16.1643 - mae: 16.6540 - val_loss: 46.1873 - val_mae: 46.6814 - learning_rate: 0.0014\n",
            "Epoch 19/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 198ms/step - loss: 15.9190 - mae: 16.4092 - val_loss: 45.8069 - val_mae: 46.3012 - learning_rate: 0.0014\n",
            "Epoch 20/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 172ms/step - loss: 15.5006 - mae: 15.9905 - val_loss: 46.4257 - val_mae: 46.9207 - learning_rate: 0.0014\n",
            "Epoch 21/30\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 183ms/step - loss: 15.0787 - mae: 15.5677 - val_loss: 46.4190 - val_mae: 46.9139 - learning_rate: 0.0014\n",
            "ğŸ¯ Aligned Model RMSE: 67.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# GENERATE PREDICTIONS (WILL WORK NOW)\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ”® Generating test predictions with aligned features...\")\n",
        "\n",
        "test_sequences = []\n",
        "for i in range(len(X_test_scaled_aligned)):\n",
        "    if i < SEQUENCE_LENGTH:\n",
        "        needed_from_train = SEQUENCE_LENGTH - (i + 1)\n",
        "        if needed_from_train > 0:\n",
        "            sequence = np.vstack([X_train_scaled_aligned[-needed_from_train:], X_test_scaled_aligned[:i+1]])\n",
        "        else:\n",
        "            sequence = X_test_scaled_aligned[:SEQUENCE_LENGTH]\n",
        "    else:\n",
        "        sequence = X_test_scaled_aligned[i-SEQUENCE_LENGTH+1:i+1]\n",
        "\n",
        "    if sequence.shape[0] != SEQUENCE_LENGTH:\n",
        "        if sequence.shape[0] < SEQUENCE_LENGTH:\n",
        "            padding_needed = SEQUENCE_LENGTH - sequence.shape[0]\n",
        "            padding = np.repeat(sequence[0:1], padding_needed, axis=0)\n",
        "            sequence = np.vstack([padding, sequence])\n",
        "        else:\n",
        "            sequence = sequence[-SEQUENCE_LENGTH:]\n",
        "\n",
        "    test_sequences.append(sequence)\n",
        "\n",
        "X_test_seq = np.array(test_sequences)\n",
        "print(f\"ğŸ“Š Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "# This will work now - same feature dimensions!\n",
        "test_predictions = model_aligned.predict(X_test_seq, verbose=0)\n",
        "test_predictions = np.maximum(test_predictions.flatten(), 0)\n",
        "\n",
        "print(f\"ğŸ“Š Predictions - Min: {test_predictions.min():.1f}, Max: {test_predictions.max():.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewGYEfVeXyWa",
        "outputId": "558e9684-f8c5-40f2-ef8f-d0cddc8b3938"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”® Generating test predictions with aligned features...\n",
            "ğŸ“Š Test sequences shape: (13148, 36, 45)\n",
            "ğŸ“Š Predictions - Min: 13.8, Max: 392.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸš€ Building and training LSTM model...\")\n",
        "\n",
        "# Create and train model\n",
        "model = create_enhanced_model(X_train.shape[1:])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.002),\n",
        "    loss='huber',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_seq,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"âœ… Model training completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1_J-DptOGx2",
        "outputId": "d765b03f-8193-424b-b075-843b84971d32"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Building and training LSTM model...\n",
            "Epoch 1/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 209ms/step - loss: 79.6651 - mae: 80.1634 - val_loss: 36.5207 - val_mae: 37.0150 - learning_rate: 0.0020\n",
            "Epoch 2/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 189ms/step - loss: 26.0483 - mae: 26.5417 - val_loss: 23.7085 - val_mae: 24.2011 - learning_rate: 0.0020\n",
            "Epoch 3/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 194ms/step - loss: 21.5637 - mae: 22.0560 - val_loss: 21.6454 - val_mae: 22.1370 - learning_rate: 0.0020\n",
            "Epoch 4/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 182ms/step - loss: 19.7368 - mae: 20.2289 - val_loss: 21.6713 - val_mae: 22.1614 - learning_rate: 0.0020\n",
            "Epoch 5/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 177ms/step - loss: 18.8892 - mae: 19.3802 - val_loss: 21.3461 - val_mae: 21.8367 - learning_rate: 0.0020\n",
            "Epoch 6/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 200ms/step - loss: 18.1183 - mae: 18.6095 - val_loss: 22.9608 - val_mae: 23.4525 - learning_rate: 0.0020\n",
            "Epoch 7/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 189ms/step - loss: 17.7294 - mae: 18.2206 - val_loss: 20.6523 - val_mae: 21.1437 - learning_rate: 0.0020\n",
            "Epoch 8/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 186ms/step - loss: 17.1730 - mae: 17.6634 - val_loss: 20.6981 - val_mae: 21.1899 - learning_rate: 0.0020\n",
            "Epoch 9/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 177ms/step - loss: 17.1246 - mae: 17.6147 - val_loss: 20.6073 - val_mae: 21.0985 - learning_rate: 0.0020\n",
            "Epoch 10/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 176ms/step - loss: 16.4996 - mae: 16.9898 - val_loss: 19.9541 - val_mae: 20.4448 - learning_rate: 0.0020\n",
            "Epoch 11/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 195ms/step - loss: 15.9852 - mae: 16.4743 - val_loss: 20.2294 - val_mae: 20.7207 - learning_rate: 0.0020\n",
            "Epoch 12/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 184ms/step - loss: 15.7902 - mae: 16.2788 - val_loss: 20.3119 - val_mae: 20.8040 - learning_rate: 0.0020\n",
            "Epoch 13/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 187ms/step - loss: 15.2533 - mae: 15.7419 - val_loss: 20.1009 - val_mae: 20.5923 - learning_rate: 0.0020\n",
            "Epoch 14/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 184ms/step - loss: 14.8443 - mae: 15.3320 - val_loss: 20.4927 - val_mae: 20.9843 - learning_rate: 0.0020\n",
            "Epoch 15/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - loss: 14.8740 - mae: 15.3626 - val_loss: 20.4455 - val_mae: 20.9368 - learning_rate: 0.0020\n",
            "Epoch 16/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 207ms/step - loss: 14.7112 - mae: 15.2001 - val_loss: 20.6145 - val_mae: 21.1051 - learning_rate: 0.0020\n",
            "Epoch 17/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 201ms/step - loss: 14.4283 - mae: 14.9172 - val_loss: 21.2159 - val_mae: 21.7075 - learning_rate: 0.0020\n",
            "Epoch 18/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 14.1564 - mae: 14.6445\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0014000000664964318.\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 193ms/step - loss: 14.1558 - mae: 14.6438 - val_loss: 20.8572 - val_mae: 21.3488 - learning_rate: 0.0020\n",
            "Epoch 19/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 303ms/step - loss: 13.8052 - mae: 14.2936 - val_loss: 20.7622 - val_mae: 21.2520 - learning_rate: 0.0014\n",
            "Epoch 20/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 255ms/step - loss: 13.5092 - mae: 13.9977 - val_loss: 21.2491 - val_mae: 21.7403 - learning_rate: 0.0014\n",
            "Epoch 21/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 206ms/step - loss: 13.2313 - mae: 13.7192 - val_loss: 20.9978 - val_mae: 21.4896 - learning_rate: 0.0014\n",
            "Epoch 22/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 201ms/step - loss: 13.0059 - mae: 13.4931 - val_loss: 21.9784 - val_mae: 22.4684 - learning_rate: 0.0014\n",
            "Epoch 23/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 204ms/step - loss: 13.2277 - mae: 13.7145 - val_loss: 21.6501 - val_mae: 22.1414 - learning_rate: 0.0014\n",
            "Epoch 24/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 194ms/step - loss: 12.9408 - mae: 13.4277 - val_loss: 21.9551 - val_mae: 22.4479 - learning_rate: 0.0014\n",
            "Epoch 25/50\n",
            "\u001b[1m204/204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 211ms/step - loss: 12.6803 - mae: 13.1668 - val_loss: 21.9211 - val_mae: 22.4129 - learning_rate: 0.0014\n",
            "âœ… Model training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“Š Evaluating model performance...\")\n",
        "\n",
        "# Evaluate model\n",
        "val_pred = model.predict(X_val, verbose=0)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "print(f\"\\nğŸ¯ Model Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Target: < 3000 RMSE\")\n",
        "\n",
        "if val_rmse < 3000:\n",
        "    print(\"ğŸ‰ TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"ğŸ“ˆ Need {val_rmse - 3000:.1f} points improvement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr6P3vWfHlCH",
        "outputId": "eada43c7-f740-4f5e-a1d5-4a3500397c64"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Evaluating model performance...\n",
            "\n",
            "ğŸ¯ Model Performance:\n",
            "Validation RMSE: 37.26\n",
            "Target: < 3000 RMSE\n",
            "ğŸ‰ TARGET ACHIEVED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CORRECT FEATURE ALIGNMENT\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ”§ Correcting feature alignment...\")\n",
        "\n",
        "# First, let's see what features we actually have\n",
        "print(\"ğŸ“Š Training features:\", train_enhanced.columns.tolist())\n",
        "print(\"ğŸ“Š Test features:\", test_enhanced.columns.tolist())\n",
        "\n",
        "# The problem: Training has PM2.5 lag features that test can't have\n",
        "# Solution: Remove PM2.5-dependent features from training\n",
        "pm25_dependent_features = [col for col in train_enhanced.columns if 'pm2.5_' in col]\n",
        "print(f\"ğŸš« Removing PM2.5-dependent features: {pm25_dependent_features}\")\n",
        "\n",
        "# Use only features that exist in both datasets\n",
        "common_features = [col for col in train_enhanced.columns\n",
        "                  if col in test_enhanced.columns and col not in ['pm2.5', 'No']]\n",
        "\n",
        "print(f\"âœ… Using {len(common_features)} common features:\")\n",
        "print(common_features)\n",
        "\n",
        "# Create aligned datasets\n",
        "X_train_aligned = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_aligned = test_enhanced[common_features]\n",
        "\n",
        "print(f\"ğŸ“Š Aligned training features: {X_train_aligned.shape}\")\n",
        "print(f\"ğŸ“Š Aligned test features: {X_test_aligned.shape}\")\n",
        "\n",
        "# Scale the aligned features\n",
        "scaler_aligned = StandardScaler()\n",
        "X_train_scaled_aligned = scaler_aligned.fit_transform(X_train_aligned)\n",
        "X_test_scaled_aligned = scaler_aligned.transform(X_test_aligned)\n",
        "\n",
        "print(\"âœ… Feature alignment completed!\")"
      ],
      "metadata": {
        "id": "0SDDX2L1VF_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c1962e-4851-4b1a-fadf-e02fd81e3fe6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Correcting feature alignment...\n",
            "ğŸ“Š Training features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ“Š Test features: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸš« Removing PM2.5-dependent features: ['pm2.5_lag_1', 'pm2.5_lag_2', 'pm2.5_lag_3', 'pm2.5_lag_6', 'pm2.5_lag_12', 'pm2.5_lag_24', 'pm2.5_lag_48', 'pm2.5_roll_mean_6', 'pm2.5_roll_std_6', 'pm2.5_roll_min_6', 'pm2.5_roll_max_6', 'pm2.5_roll_mean_12', 'pm2.5_roll_std_12', 'pm2.5_roll_min_12', 'pm2.5_roll_max_12', 'pm2.5_roll_mean_24', 'pm2.5_roll_std_24', 'pm2.5_roll_min_24', 'pm2.5_roll_max_24', 'pm2.5_roll_mean_48', 'pm2.5_roll_std_48', 'pm2.5_roll_min_48', 'pm2.5_roll_max_48']\n",
            "âœ… Using 45 common features:\n",
            "['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'season', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'temp_dewp_diff', 'wind_pressure', 'humidity_index', 'DEWP_roll_mean_6', 'DEWP_roll_std_6', 'DEWP_roll_mean_12', 'DEWP_roll_std_12', 'DEWP_roll_mean_24', 'DEWP_roll_std_24', 'TEMP_roll_mean_6', 'TEMP_roll_std_6', 'TEMP_roll_mean_12', 'TEMP_roll_std_12', 'TEMP_roll_mean_24', 'TEMP_roll_std_24', 'PRES_roll_mean_6', 'PRES_roll_std_6', 'PRES_roll_mean_12', 'PRES_roll_std_12', 'PRES_roll_mean_24', 'PRES_roll_std_24', 'Iws_roll_mean_6', 'Iws_roll_std_6', 'Iws_roll_mean_12', 'Iws_roll_std_12', 'Iws_roll_mean_24', 'Iws_roll_std_24', 'wind_complexity']\n",
            "ğŸ“Š Aligned training features: (30676, 45)\n",
            "ğŸ“Š Aligned test features: (13148, 45)\n",
            "âœ… Feature alignment completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# CREATE SUBMISSION\n",
        "# ======================\n",
        "\n",
        "print(\"ğŸ’¾ Creating submission file...\")\n",
        "\n",
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ¯ EXPERIMENT COMPLETE!\")\n",
        "print(f\"ğŸ“ˆ Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"ğŸ’¾ Submission saved: {filename}\")\n",
        "print(f\"\\nğŸ“‹ Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"âœ… Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"ğŸ“ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"âŒ Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-U4WGAH1sE",
        "outputId": "ae9dbc33-a0a3-43ce-8407-72f10ab6b2c5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Creating submission file...\n",
            "âœ… Submission saved: submissions/20250920_092621_enhanced_lstm_rmse_37.csv\n",
            "ğŸ“Š Predictions - Min: 13.8, Max: 392.3\n",
            "\n",
            "ğŸ¯ EXPERIMENT COMPLETE!\n",
            "ğŸ“ˆ Validation RMSE: 37.26\n",
            "ğŸ’¾ Submission saved: submissions/20250920_092621_enhanced_lstm_rmse_37.csv\n",
            "\n",
            "ğŸ“‹ Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0   2013-07-02 4:00:00     31\n",
            "1   2013-07-02 5:00:00     30\n",
            "2   2013-07-02 6:00:00     29\n",
            "3   2013-07-02 7:00:00     28\n",
            "4   2013-07-02 8:00:00     28\n",
            "5   2013-07-02 9:00:00     27\n",
            "6  2013-07-02 10:00:00     25\n",
            "7  2013-07-02 11:00:00     25\n",
            "8  2013-07-02 12:00:00     24\n",
            "9  2013-07-02 13:00:00     24\n",
            "âœ… Confirmed: submissions/20250920_092621_enhanced_lstm_rmse_37.csv exists in submissions folder!\n",
            "ğŸ“ File size: 301342 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ¯ EXPERIMENT COMPLETE!\")\n",
        "print(f\"ğŸ“ˆ Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"ğŸ’¾ Submission saved: {filename}\")\n",
        "print(f\"\\nğŸ“‹ Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"âœ… Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"ğŸ“ File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"âŒ Warning: {filename} not found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_YV-N-gJBAv",
        "outputId": "01f03bf2-d7b2-449d-9e38-aa9cfc540e96"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Submission saved: submissions/20250920_092625_enhanced_lstm_rmse_37.csv\n",
            "ğŸ“Š Predictions - Min: 13.8, Max: 392.3\n",
            "\n",
            "ğŸ¯ EXPERIMENT COMPLETE!\n",
            "ğŸ“ˆ Validation RMSE: 37.26\n",
            "ğŸ’¾ Submission saved: submissions/20250920_092625_enhanced_lstm_rmse_37.csv\n",
            "\n",
            "ğŸ“‹ Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0   2013-07-02 4:00:00     31\n",
            "1   2013-07-02 5:00:00     30\n",
            "2   2013-07-02 6:00:00     29\n",
            "3   2013-07-02 7:00:00     28\n",
            "4   2013-07-02 8:00:00     28\n",
            "5   2013-07-02 9:00:00     27\n",
            "6  2013-07-02 10:00:00     25\n",
            "7  2013-07-02 11:00:00     25\n",
            "8  2013-07-02 12:00:00     24\n",
            "9  2013-07-02 13:00:00     24\n",
            "âœ… Confirmed: submissions/20250920_092625_enhanced_lstm_rmse_37.csv exists in submissions folder!\n",
            "ğŸ“ File size: 301342 bytes\n"
          ]
        }
      ]
    }
  ]
}